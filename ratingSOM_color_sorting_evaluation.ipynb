{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3,  6, 10, 15, 21, 28,  2,  4,  7, 11, 16, 22, 29, 36,  5,\n",
       "        8, 12, 17, 23, 30, 37, 43,  9, 13, 18, 24, 31, 38, 44, 49, 14, 19,\n",
       "       25, 32, 39, 45, 50, 54, 20, 26, 33, 40, 46, 51, 55, 58, 27, 34, 41,\n",
       "       47, 52, 56, 59, 61, 35, 42, 48, 53, 57, 60, 62, 63])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getGoldenTriangleOrdering(size):\n",
    "    #get ordering of the golden triangle starting from top-left corner and going over second diagonal.\n",
    "    #having a 3x3 square, the ordering is [0,0], [0,1], [1,0], [0,2], [1,1], [2,0], [1,2], [2,1], [2,2]\n",
    "    maxIndexSum = size*2 -1\n",
    "    listOfIndices = []\n",
    "    \n",
    "    for n in range(maxIndexSum):\n",
    "        for i in range(size):\n",
    "            firstIndex = i\n",
    "            secondIndex = n-i\n",
    "            if secondIndex < size and secondIndex >= 0:\n",
    "                listOfIndices.append((firstIndex,secondIndex))\n",
    "    \n",
    " \n",
    "    mat = np.zeros((size,size),dtype=int)   \n",
    "    vals = np.array(range(size**2))\n",
    "\n",
    "    for i,idx in enumerate(listOfIndices):\n",
    "        mat[idx] = vals[i]\n",
    "    return mat\n",
    "indexMatrix = getGoldenTriangleOrdering(8)\n",
    "indexMatrix.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating-aware SOM class\n",
    "- providing self-organizing map for given dataset\n",
    "- variants both with / without rating-awareness and with / without probabilistic sampling\n",
    "- the main added feature is _find_bmu_rank_aware() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to implement simple self organizing map using PyTorch, with methods\n",
    "similar to clustering method in sklearn.\n",
    "@author: Riley Smith\n",
    "Created: 1-27-21\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class SOM():\n",
    "    \"\"\"\n",
    "    The 2-D, rectangular grid self-organizing map class using Numpy.\n",
    "    \"\"\"\n",
    "    def __init__(self, m=3, n=3, dim=3, lr=1, sigma=1, max_iter=3000,\n",
    "                    random_state=None, som_type=\"normal\", alpha=0.5, use_triangular=False, use_prob_candidate_selection = False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        m : int, default=3\n",
    "            The shape along dimension 0 (vertical) of the SOM.\n",
    "        n : int, default=3\n",
    "            The shape along dimesnion 1 (horizontal) of the SOM.\n",
    "        dim : int, default=3\n",
    "            The dimensionality (number of features) of the input space.\n",
    "        lr : float, default=1\n",
    "            The initial step size for updating the SOM weights.\n",
    "        sigma : float, optional\n",
    "            Optional parameter for magnitude of change to each weight. Does not\n",
    "            update over training (as does learning rate). Higher values mean\n",
    "            more aggressive updates to weights.\n",
    "        max_iter : int, optional\n",
    "            Optional parameter to stop training if you reach this many\n",
    "            interation.\n",
    "        random_state : int, optional\n",
    "            Optional integer seed to the random number generator for weight\n",
    "            initialization. This will be used to create a new instance of Numpy's\n",
    "            default random number generator (it will not call np.random.seed()).\n",
    "            Specify an integer for deterministic results.\n",
    "        som_type : string, optional\n",
    "            Options to determine whether classical (normal), rating-aware (rating), rank-aware (rank), or rank aware\n",
    "            with positional discounts (rank_pos) SOM is performed\n",
    "        alpha: float, optional\n",
    "            Hyperparameter to tune importance of ranking vs. local similarity. \n",
    "            Higher alpha values denote more importance to the local similarity (original SOM)\n",
    "        use_triangular: bool, optional\n",
    "            Determine the ranking of displayed grid coordinates: row-first bases (False) \n",
    "            or triangular starting from top-left corner (True)    \n",
    "        use_prob_candidate_selection: bool, optional\n",
    "            For each epoch, candidates are either selected uniformly (False) or based on probability distribution (True)\n",
    "            Probability distribution is supposed to be induced by the rating of individual candidates\n",
    "            (better rating => higher probability to affect SOM composition)\n",
    "        \"\"\"\n",
    "        # Initialize descriptive features of SOM\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.shape = (m, n)\n",
    "        self.initial_lr = lr\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        \n",
    "        #initialize rankingSOM specific features\n",
    "        self.som_type = som_type\n",
    "        self.alpha = alpha\n",
    "        self.use_triangular = use_triangular\n",
    "        self.use_prob_candidate_selection = use_prob_candidate_selection\n",
    "        self.triangular_ordering = self._get_golden_triangle_ordering(m).reshape(-1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.random_state = random_state\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        self.weights = rng.normal(size=(m * n, dim))\n",
    "        self._locations = self._get_locations(m, n)\n",
    "\n",
    "        # Set after fitting\n",
    "        self._inertia = None\n",
    "        self._n_iter_ = None\n",
    "        self._trained = False\n",
    "        \n",
    "        \n",
    "    def _get_golden_triangle_ordering(self, size):\n",
    "        #get ordering of the golden triangle starting from top-left corner and going over second diagonal.\n",
    "        #having a 3x3 square, the ordering is [0,0], [0,1], [1,0], [0,2], [1,1], [2,0], [1,2], [2,1], [2,2]\n",
    "        #only works for squares (extension for rectangles plausible TODO)\n",
    "        maxIndexSum = size*2 -1\n",
    "        listOfIndices = []\n",
    "\n",
    "        for n in range(maxIndexSum):\n",
    "            for i in range(size):\n",
    "                firstIndex = i\n",
    "                secondIndex = n-i\n",
    "                if secondIndex < size and secondIndex >= 0:\n",
    "                    listOfIndices.append((firstIndex,secondIndex))\n",
    "\n",
    "\n",
    "        mat = np.zeros((size,size),dtype=int)   \n",
    "        vals = np.array(range(size**2))\n",
    "\n",
    "        for i,idx in enumerate(listOfIndices):\n",
    "            mat[idx] = vals[i]\n",
    "        return mat\n",
    "\n",
    "\n",
    "    def _get_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Return the indices of an m by n array.\n",
    "        \"\"\"\n",
    "        return np.argwhere(np.ones(shape=(m, n))).astype(np.int64)\n",
    "\n",
    "    def _find_bmu(self, x):\n",
    "        \"\"\"\n",
    "        Find the index of the best matching unit for the input vector x.\n",
    "        \"\"\"\n",
    "        # Stack x to have one row per weight\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "        # Calculate distance between x and each weight\n",
    "        distance = np.linalg.norm(x_stack - self.weights, axis=1)\n",
    "        # Find index of best matching unit\n",
    "        return np.argmin(distance)\n",
    "\n",
    "    def _find_bmu_rank_aware(self, x, rank, rating):\n",
    "        \"\"\"\n",
    "        Consider ranking of individual cases\n",
    "        Directly optimizing Kendall Tau coefficient\n",
    "        Approximative normalization due to comparability between both distance metrics\n",
    "        Possible problem: too small difference of rank-aware distances if m*n is small compared to total volume of samples\n",
    "        \"\"\"\n",
    "        # Stack x to have one row per weight\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "        # Calculate distance between x and each weight\n",
    "        distance = np.linalg.norm(x_stack - self.weights, axis=1)\n",
    "        # Find index of best matching unit\n",
    "        \n",
    "        x_rank_stack = np.array([rank+1]*(self.m*self.n))\n",
    "        \n",
    "        # Use row-wise or left-top triangular ordering of grid\n",
    "        if self.use_triangular == True:\n",
    "            nodes_rank = self.triangular_ordering+1\n",
    "        else:            \n",
    "            nodes_rank = np.array(range((self.m*self.n)))+1\n",
    "        \n",
    "        rankDistance = np.absolute(x_rank_stack-nodes_rank) / self.volume_of_samples\n",
    "        \n",
    "        finalDistance = self.alpha * distance + (1-self.alpha) * rankDistance\n",
    "        return np.argmin(finalDistance)\n",
    "    \n",
    "    \n",
    "    def _find_bmu_rank_aware_positional(self, x, rank, rating):\n",
    "        \"\"\"\n",
    "        Consider ranking of individual cases\n",
    "        Rather drastic increase in importance w.r.t. position of item (more important on top positions than later on)\n",
    "        Does not optimize any well-known metric in particular, but may provide visually reasonable results\n",
    "        \"\"\"\n",
    "        # Stack x to have one row per weight\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "        # Calculate distance between x and each weight\n",
    "        distance = np.linalg.norm(x_stack - self.weights, axis=1)\n",
    "        # Find index of best matching unit\n",
    "        \n",
    "        x_rank_stack = np.array([rank+1]*(self.m*self.n))\n",
    "        \n",
    "        # Use row-wise or left-top triangular ordering of grid\n",
    "        if self.use_triangular == True:\n",
    "            nodes_rank = self.triangular_ordering+1\n",
    "        else:            \n",
    "            nodes_rank = np.array(range((self.m*self.n)))+1\n",
    "        \n",
    "        # the part of denominator self.volume_of_samples/(self.m*self.n) is not necessary\n",
    "        # it is kept to normalize the results to the _find_bmu_rank_aware() function (the last node has the same penalty)\n",
    "        rankDistance = np.absolute(x_rank_stack-nodes_rank) / (nodes_rank * self.volume_of_samples/(self.m*self.n))\n",
    "        \n",
    "        finalDistance = self.alpha * distance + (1-self.alpha) * rankDistance\n",
    "        return np.argmin(finalDistance)\n",
    "    \n",
    "    \n",
    "    def _find_bmu_rating_aware(self, x, rank, rating):\n",
    "        \"\"\"\n",
    "        Consider ratings of individual cases\n",
    "        Optimize piece-wise DCG (i.e., difference between expected relevance on this position and the actual one)\n",
    "        Importance of the distance for particular field is weighted in the same way as in nDCG (1/log2(pos+1))\n",
    "        \"\"\"\n",
    "        \n",
    "        # Stack x to have one row per weight\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "        # Calculate distance between x and each weight\n",
    "        distance = np.linalg.norm(x_stack - self.weights, axis=1)\n",
    "        # Find index of best matching unit\n",
    "        \n",
    "        x_rating_stack = np.array([rating]*(self.m*self.n))   \n",
    "        \n",
    "        # Use row-wise or left-top triangular ordering of grid\n",
    "        if self.use_triangular == True:\n",
    "            nodes_rank = self.triangular_ordering+1\n",
    "        else:            \n",
    "            nodes_rank = np.array(range((self.m*self.n)))+1\n",
    "            \n",
    "        rankBasedWeights = 1/np.log2(nodes_rank+1)       \n",
    "        rankDistance = rankBasedWeights * np.absolute(x_rating_stack-self.expected_ratings)\n",
    "        \n",
    "        finalDistance = self.alpha * distance + (1-self.alpha) * rankDistance\n",
    "        return np.argmin(finalDistance)\n",
    "        \n",
    "    \n",
    "    def step(self, x, rank, rating):\n",
    "        \"\"\"\n",
    "        Do one step of training on the given input vector.\n",
    "        \"\"\"\n",
    "        # Stack x to have one row per weight\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "\n",
    "        # Get index of best matching unit\n",
    "        # Based on the SOM type, different procedures are applied here\n",
    "        # - this is the only difference between normal SOM and rank-aware SOM\n",
    "        \n",
    "        #classical (normal), rating-aware (rating), rank-aware (rank), or rank aware with positional discounts (rank_pos)\n",
    "        if self.som_type == \"normal\":\n",
    "            bmu_index = self._find_bmu(x)\n",
    "        elif self.som_type == \"rating\":\n",
    "            bmu_index = self._find_bmu_rating_aware(x, rank, rating)        \n",
    "        elif self.som_type == \"rank\":\n",
    "            bmu_index = self._find_bmu_rank_aware(x, rank, rating)\n",
    "        elif self.som_type == \"rank_pos\":\n",
    "            bmu_index = self._find_bmu_rank_aware_positional(x, rank, rating)            \n",
    "            \n",
    "        # Find location of best matching unit\n",
    "        bmu_location = self._locations[bmu_index,:]\n",
    "\n",
    "        # Find square distance from each weight to the BMU\n",
    "        stacked_bmu = np.stack([bmu_location]*(self.m*self.n), axis=0)\n",
    "        bmu_distance = np.sum(np.power(self._locations.astype(np.float64) - stacked_bmu.astype(np.float64), 2), axis=1)\n",
    "\n",
    "        # Compute update neighborhood\n",
    "        neighborhood = np.exp((bmu_distance / (self.sigma ** 2)) * -1)\n",
    "        local_step = self.lr * neighborhood\n",
    "\n",
    "        # Stack local step to be proper shape for update\n",
    "        local_multiplier = np.stack([local_step]*(self.dim), axis=1)\n",
    "\n",
    "        # Multiply by difference between input and weights\n",
    "        delta = local_multiplier * (x_stack - self.weights)\n",
    "\n",
    "        # Update weights\n",
    "        self.weights += delta\n",
    "\n",
    "    def _compute_point_intertia(self, x):\n",
    "        \"\"\"\n",
    "        Compute the inertia of a single point. Inertia defined as squared distance\n",
    "        from point to closest cluster center (BMU)\n",
    "        \"\"\"\n",
    "        # Find BMU\n",
    "        bmu_index = self._find_bmu(x)\n",
    "        bmu = self.weights[bmu_index]\n",
    "        # Compute sum of squared distance (just euclidean distance) from x to bmu\n",
    "        return np.sum(np.square(x - bmu))\n",
    "\n",
    "    def fit(self, X, ranks, ratings, epochs=1, shuffle=True):\n",
    "        \"\"\"\n",
    "        Take data (a tensor of type float64) as input and fit the SOM to that\n",
    "        data for the specified number of epochs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Training data. Must have shape (n, self.dim) where n is the number\n",
    "            of training samples.\n",
    "        ranks: ndarray\n",
    "            Ranks of the training data w.r.t. original ordering\n",
    "        ratings: ndarray\n",
    "            Ratings of the training data w.r.t. original ordering\n",
    "        epochs : int, default=1\n",
    "            The number of times to loop through the training data when fitting.\n",
    "        shuffle : bool, default True\n",
    "            Whether or not to randomize the order of train data when fitting.\n",
    "            Can be seeded with np.random.seed() prior to calling fit.\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            Fits the SOM to the given data but does not return anything.\n",
    "        \"\"\"\n",
    "        topK = np.argsort(ranks)[:(self.m*self.n)]\n",
    "        self.expected_ratings = ratings[topK]\n",
    "        self.volume_of_samples = len(X)\n",
    "        \n",
    "        # Count total number of iterations\n",
    "        global_iter_counter = 0\n",
    "        n_samples = X.shape[0]\n",
    "        total_iterations = np.minimum(epochs * n_samples, self.max_iter)\n",
    "        \n",
    "        #prepare for probabilistic candidates selection\n",
    "        rngForCandidateSelection = np.random.default_rng(self.random_state)\n",
    "        prob = ratings/np.sum(ratings)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Break if past max number of iterations\n",
    "            if global_iter_counter > self.max_iter:\n",
    "                break\n",
    "            \n",
    "            if self.use_prob_candidate_selection:\n",
    "                indices = rngForCandidateSelection.choice(np.arange(n_samples) , size=n_samples, p=prob)             \n",
    "            elif shuffle:\n",
    "                rng = np.random.default_rng(self.random_state)\n",
    "                indices = rng.permutation(n_samples)\n",
    "            else:\n",
    "                indices = np.arange(n_samples)\n",
    "\n",
    "            # Train\n",
    "            for idx in indices:\n",
    "                # Break if past max number of iterations\n",
    "                if global_iter_counter > self.max_iter:\n",
    "                    break\n",
    "                input = X[idx]\n",
    "                rank = ranks[idx]\n",
    "                rating = ratings[idx]\n",
    "                # Do one step of training\n",
    "                self.step(input, rank, rating)\n",
    "                # Update learning rate\n",
    "                global_iter_counter += 1\n",
    "                self.lr = (1 - (global_iter_counter / total_iterations)) * self.initial_lr\n",
    "\n",
    "        # Compute inertia\n",
    "        inertia = np.sum(np.array([float(self._compute_point_intertia(x)) for x in X]))\n",
    "        self._inertia_ = inertia\n",
    "\n",
    "        # Set n_iter_ attribute\n",
    "        self._n_iter_ = global_iter_counter\n",
    "\n",
    "        # Set trained flag\n",
    "        self._trained = True\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, X, ranks, ratings):\n",
    "        \"\"\"\n",
    "        Predict cluster for each element in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            An ndarray of shape (n, self.dim) where n is the number of samples.\n",
    "            The data to predict clusters for.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : ndarray\n",
    "            An ndarray of shape (n,). The predicted cluster index for each item\n",
    "            in X.\n",
    "        \"\"\"\n",
    "        # Check to make sure SOM has been fit\n",
    "        if not self._trained:\n",
    "            raise NotImplementedError('SOM object has no predict() method until after calling fit().')\n",
    "\n",
    "        # Make sure X has proper shape\n",
    "        assert len(X.shape) == 2, f'X should have two dimensions, not {len(X.shape)}'\n",
    "        assert X.shape[1] == self.dim, f'This SOM has dimesnion {self.dim}. Received input with dimension {X.shape[1]}'\n",
    "        \n",
    "        \n",
    "        indices = range(len(ranks))\n",
    "        labels = np.array([self._find_bmu(X[idx,:]) for idx in indices])\n",
    "        #if self.som_type == \"normal\":\n",
    "        #    labels = np.array([self._find_bmu(X[idx,:]) for idx in indices])\n",
    "        #elif self.som_type == \"rank\":\n",
    "        #    labels = np.array([self._find_bmu_rank_aware(X[idx,:], ranks[idx], ratings[idx]) for idx in indices])\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def transform(self, X, ranks, ratings):\n",
    "        \"\"\"\n",
    "        Transform the data X into cluster distance space.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Data of shape (n, self.dim) where n is the number of samples. The\n",
    "            data to transform.\n",
    "        Returns\n",
    "        -------\n",
    "        transformed : ndarray\n",
    "            Transformed data of shape (n, self.n*self.m). The Euclidean distance\n",
    "            from each item in X to each cluster center.\n",
    "        \"\"\"\n",
    "        # Stack data and cluster centers\n",
    "        X_stack = np.stack([X]*(self.m*self.n), axis=1)\n",
    "        cluster_stack = np.stack([self.weights]*X.shape[0], axis=0)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = X_stack - cluster_stack\n",
    "\n",
    "        # Take and return norm\n",
    "        return np.linalg.norm(diff, axis=2)\n",
    "\n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        return self.weights.reshape(self.m, self.n, self.dim)\n",
    "\n",
    "    @property\n",
    "    def inertia_(self):\n",
    "        if self._inertia_ is None:\n",
    "            raise AttributeError('SOM does not have inertia until after calling fit()')\n",
    "        return self._inertia_\n",
    "\n",
    "    @property\n",
    "    def n_iter_(self):\n",
    "        if self._n_iter_ is None:\n",
    "            raise AttributeError('SOM does not have n_iter_ attribute until after calling fit()')\n",
    "        return self._n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation process class\n",
    "responsible for color definition & selection and running Rating-aware SOM + all baselines on selected test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalProcess():\n",
    "    \n",
    "    labels = {\n",
    "        \"normal\":\"Plain SOM\",\n",
    "        \"rating\":\"Rating SOM\",\n",
    "        \"rank\":\"Rank SOM\",\n",
    "        \"rank_pos\":\"DiscRank SOM\",\n",
    "        \"topk\":\"Top-K display\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    def __init__(self, som_variants, alphas,  display_mode, exponents, vol_of_examples, \n",
    "                 test_set_size, bias_std_dev, bias_std_devTarget, use_triangular):          \n",
    "        self.SOM_dim = 8\n",
    "        \n",
    "        self.som_variants = som_variants\n",
    "        self.alphas = alphas\n",
    "        self.display_mode = display_mode\n",
    "        self.exponents = exponents\n",
    "        self.examples = vol_of_examples\n",
    "        \n",
    "        self.test_size = test_set_size\n",
    "        self.bias_std_dev = bias_std_dev\n",
    "        self.bias_std_devTarget = bias_std_devTarget\n",
    "        self.use_triangular = use_triangular\n",
    "        self.use_prob_candidate_selections = [False,True]\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        sampleColors = np.random.choice(range(0,255), size=(3*self.examples))\n",
    "        self.unbiasedSamples = sampleColors.reshape((-1,3))/255\n",
    "        \n",
    "        np.random.seed(24)\n",
    "        testTargets = np.random.choice(range(0,255), size=(3*self.test_size))\n",
    "        self.testTargets = testTargets.reshape((-1,3))/255\n",
    "        \n",
    "    def _evaluateTestSampleRating(self):\n",
    "        \"\"\"\n",
    "        evaluates rating and ranking between sample objects and test objects\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(42)\n",
    "        samples = self.unbiasedSamples\n",
    "        \n",
    "        self.pairwiseSampleDistances = euclidean_distances(samples.reshape(-1,3)) \n",
    "        unbiasedDistance = euclidean_distances(samples.reshape(-1,3), self.testTargets.reshape(-1,3)) \n",
    "        \n",
    "        self.unbiasedRatings = np.exp(-self.exponent*unbiasedDistance)\n",
    "        #self.unbiasedRatings = ((2-unbiasedDistance)/2)**self.exponent\n",
    "        \n",
    "        if self.bias_std_dev >0:\n",
    "            #bias = np.random.normal(0.0, self.bias_std_dev, size=self.unbiasedSamples.shape)\n",
    "            #samples = self.unbiasedSamples+bias\n",
    "        \n",
    "            bias = np.random.normal(0.0, self.bias_std_devTarget, size=self.testTargets.shape)\n",
    "            testTargets = self.testTargets+bias\n",
    "            \n",
    "        self.samples = samples #using biased samples to order stuf\n",
    "        \n",
    "        distance = euclidean_distances(samples.reshape(-1,3), testTargets.reshape(-1,3))        \n",
    "\n",
    "        self.ratings = np.exp(-self.exponent*distance)\n",
    "        #self.ratings = ((2-distance)/2)**self.exponent\n",
    "        \n",
    "        ranks = []\n",
    "        temp = distance.argsort(axis=0)\n",
    "        for i in range(self.test_size):\n",
    "            rank = np.empty_like(temp[:,i])\n",
    "\n",
    "            rank[temp[:,i]] = np.arange(distance.shape[0])\n",
    "            ranks.append(rank)\n",
    "        self.ranks = np.stack(ranks, axis=-1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _evaluateTestSampleRatingCosine(self):\n",
    "        \"\"\"\n",
    "        evaluates rating and ranking between sample objects and test objects\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        np.random.seed(42)\n",
    "        samples = self.samples\n",
    "        \n",
    "        self.pairwiseSampleDistances = cosine_distances(samples.reshape(-1,3)) \n",
    "        unbiasedDistance = cosine_distances(samples.reshape(-1,3), self.testTargets.reshape(-1,3)) \n",
    "        \n",
    "        self.unbiasedRatings = np.exp(-self.exponent*unbiasedDistance)\n",
    "        #self.unbiasedRatings = ((2-unbiasedDistance)/2)**self.exponent\n",
    "        \n",
    "        if self.bias_std_dev >0:\n",
    "            bias = np.random.normal(0.0, self.bias_std_dev, size=self.samples.shape)\n",
    "            samples = self.samples+bias\n",
    "       \n",
    "        distance = cosine_distances(samples.reshape(-1,3), self.testTargets.reshape(-1,3))        \n",
    "\n",
    "        self.ratings = np.exp(-self.exponent*distance)\n",
    "        #self.ratings = ((2-distance)/2)**self.exponent\n",
    "        \n",
    "        ranks = []\n",
    "        temp = distance.argsort(axis=0)\n",
    "        for i in range(self.test_size):\n",
    "            rank = np.empty_like(temp[:,i])\n",
    "\n",
    "            rank[temp[:,i]] = np.arange(distance.shape[0])\n",
    "            ranks.append(rank)\n",
    "        self.ranks = np.stack(ranks, axis=-1)\n",
    "        \n",
    "        pairwiseDistances = cosine_distances(samples.reshape(-1,3)) \n",
    "\n",
    "    def displayOneElement(self, ax, x, y, color):\n",
    "        #rect = patches.Rectangle((x, y), 1, 1, linewidth=1, edgecolor='none', facecolor=color)\n",
    "        #ax.add_patch(rect)\n",
    "        pass\n",
    "    \n",
    "    def finalizeImage(self, ax, som_variant):\n",
    "        ax.set_ylim(0, self.SOM_dim)\n",
    "        ax.set_xlim(0, self.SOM_dim)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_title(self.labels[som_variant])\n",
    "        \n",
    "    def displayTopKForRankSOM(self, ax, target, ratings, rankings, som_type, alpha, use_triangular, use_prob_candidate_selection):\n",
    "\n",
    "        color_som = SOM(m=self.SOM_dim, n=self.SOM_dim, dim=3, random_state=42, max_iter=10000, \n",
    "                        som_type=som_type, alpha=alpha, use_triangular=use_triangular, \n",
    "                        use_prob_candidate_selection = use_prob_candidate_selection)        \n",
    "        color_som.fit(self.samples, rankings, ratings, epochs=10)\n",
    "        \n",
    "        linearized_indexes = color_som.predict(self.samples, rankings, ratings)\n",
    "        indexes = np.unravel_index(linearized_indexes,(self.SOM_dim,self.SOM_dim))\n",
    "        \n",
    "        allGridPositions = np.arange(self.SOM_dim**2)\n",
    "        \n",
    "        idxToDisplay = []\n",
    "        for i in allGridPositions:\n",
    "            clusterGroup = (linearized_indexes == i).nonzero()[0]\n",
    "            clusterGroup = [i for i in clusterGroup if i not in idxToDisplay]\n",
    "            if len(clusterGroup) > 0:     \n",
    "                #non-empty cluster, get top-k element\n",
    "                idx = np.arange(ratings.size)[clusterGroup]\n",
    "                idxToDisplay.append(idx[np.argmax(ratings[idx])])\n",
    "            else:\n",
    "                #empty cluster, finding closest not yet used image to the cluster centre\n",
    "                clusterCentre = color_som.weights[i]\n",
    "                distanceToClusterCentre = euclidean_distances(self.samples.reshape(-1,3), clusterCentre.reshape(-1,3)).reshape(-1)\n",
    "                sortedDistances = np.argsort(distanceToClusterCentre) \n",
    "                sortedDistances = [i for i in sortedDistances if i not in idxToDisplay]\n",
    "                idxToDisplay.append(sortedDistances[0])\n",
    "        #print(np.unique(idxToDisplay).shape)\n",
    "        \n",
    "        displayedIDXs = []\n",
    "        displayedRanks = []\n",
    "        displayedRatings = []\n",
    "        \n",
    "        for i, idx in enumerate(idxToDisplay):\n",
    "            y = self.SOM_dim - (i // self.SOM_dim) -1\n",
    "            x = i % self.SOM_dim  \n",
    "            color = self.samples[idx]\n",
    "            \n",
    "            displayedIDXs.append(idx)\n",
    "            displayedRanks.append(rankings[idx])\n",
    "            displayedRatings.append(ratings[idx])\n",
    "            \n",
    "            self.displayOneElement(ax, x, y, color)            \n",
    "\n",
    "        return displayedIDXs,displayedRanks,displayedRatings        \n",
    "        \n",
    "\n",
    "        \n",
    "    def displayTopKForTopK(self, ax, target, ratings, rankings, som_type, alpha, use_triangular, use_prob_candidate_selection):\n",
    "        topkIndeces = np.argsort(rankings)[0:(self.SOM_dim*self.SOM_dim)]\n",
    "        if use_triangular:\n",
    "            ordering = getGoldenTriangleOrdering(self.SOM_dim).reshape(-1)\n",
    "            topkIndeces = topkIndeces[ordering]\n",
    "            \n",
    "        displayedIDXs = []\n",
    "        displayedRanks = []\n",
    "        displayedRatings = []\n",
    "            \n",
    "        for i,idx in enumerate(topkIndeces):\n",
    "            y = self.SOM_dim - (i // self.SOM_dim) -1\n",
    "            x = i % self.SOM_dim            \n",
    "            color = self.samples[idx]\n",
    "            \n",
    "            displayedIDXs.append(idx)\n",
    "            displayedRanks.append(rankings[idx])\n",
    "            displayedRatings.append(ratings[idx])\n",
    "\n",
    "            self.displayOneElement(ax, x, y, color)\n",
    "\n",
    "\n",
    "        return displayedIDXs,displayedRanks,displayedRatings       \n",
    "        \n",
    "    def runConfigurations(self): \n",
    "        resultingIDX = {}\n",
    "        ratings = {}\n",
    "        ratingsUnbiased = {}\n",
    "        for e in self.exponents:\n",
    "            self.exponent = e\n",
    "            # calculate rating and ranking for individual test cases and sample data\n",
    "            self._evaluateTestSampleRating()\n",
    "            \n",
    "            for a in self.alphas:\n",
    "                print(\"alpha: \"+str(a)+\" exponent:\"+str(e))\n",
    "                for i in range(self.test_size):\n",
    "                    if i%10 == 0:\n",
    "                        print(\"test-case: \"+str(i))\n",
    "                    for pcs in self.use_prob_candidate_selections:\n",
    "                        self.use_prob_candidate_selection = pcs\n",
    "                    \n",
    "\n",
    "                        \n",
    "                        #numOfVariants = len(self.som_variants)\n",
    "\n",
    "                        #figWidth = 1+2*numOfVariants\n",
    "                        #widthRatios = [1]+[2]*numOfVariants\n",
    "                        #ax = np.zeros(len(self.som_variants)+2)\n",
    "                        #fig, ax = plt.subplots(1, numOfVariants+1, figsize=(figWidth,2), gridspec_kw={'width_ratios': widthRatios}) \n",
    "\n",
    "                        #ax[0].imshow(self.testTargets[i].reshape(1,-1,3))\n",
    "                        #ax[0].set_xticklabels([])\n",
    "                        #ax[0].set_yticklabels([])\n",
    "                        #ax[0].set_title(\"Target\")\n",
    "                        #ax[0].set_anchor(\"N\")\n",
    "\n",
    "                        for idx,sv in enumerate(self.som_variants):\n",
    "                            if a > self.alphas[0] and (sv == \"normal\" or sv ==\"topk\"):\n",
    "                                continue #run plain som and topk only once\n",
    "                            if e != self.exponents[2] and pcs == False:\n",
    "                                continue # only one variant of exponent for non-probabilistic approaches\n",
    "\n",
    "                            #currAxis = ax[idx+1]\n",
    "                            currAxis = \"\"\n",
    "                            (displayedIDXs,displayedRatings,displayedUnbiasedRatings) = self.runOneStep(currAxis, a, sv, i)\n",
    "                            resultingIDX[(a,i,sv,e,self.display_mode,self.use_triangular,self.use_prob_candidate_selection)] = displayedIDXs\n",
    "                            ratings[(a,i,sv,e,self.display_mode,self.use_triangular,self.use_prob_candidate_selection)] = displayedRatings\n",
    "                            ratingsUnbiased[(a,i,sv,e,self.display_mode,self.use_triangular,self.use_prob_candidate_selection)] = displayedUnbiasedRatings\n",
    "\n",
    "                        #plt.tight_layout()\n",
    "                        #imgName = \"fig_alpha\"+str(a)+\"_target\"+str(i)+\"_disp\"+str(self.display_mode)+\"_triag\"+str(self.use_triangular)+\"_rndTrain\"+str(evalProc.use_prob_candidate_selection)\n",
    "                        #plt.savefig(\"img/\"+imgName+\".png\", dpi=300)\n",
    "                        #plt.close()\n",
    "        return (resultingIDX,ratings,ratingsUnbiased)\n",
    "              \n",
    "            \n",
    "    def runOneStep(self, ax, alpha, som_variant, testIndex):   \n",
    "        if self.display_mode == \"topk\":   \n",
    "            if som_variant == \"topk\":\n",
    "                (displayedIDXs,displayedRanks,displayedRatings) = self.displayTopKForTopK(\n",
    "                    ax, self.testTargets[testIndex], self.ratings[:,testIndex], \n",
    "                    self.ranks[:,testIndex], som_variant, alpha, self.use_triangular, self.use_prob_candidate_selection)                \n",
    "            else:\n",
    "                (displayedIDXs,displayedRanks,displayedRatings) = self.displayTopKForRankSOM(\n",
    "                    ax, self.testTargets[testIndex], self.ratings[:,testIndex], \n",
    "                    self.ranks[:,testIndex], som_variant, alpha, self.use_triangular, self.use_prob_candidate_selection)\n",
    "                    \n",
    "            #self.finalizeImage(ax, som_variant) \n",
    "            displayedUnbiasedRatings = self.unbiasedRatings[displayedIDXs,testIndex]\n",
    "            \n",
    "        return (displayedIDXs,displayedRatings,displayedUnbiasedRatings)\n",
    "     \n",
    "        \n",
    "def processData(dataDict):\n",
    "        resultsDF = pd.DataFrame.from_dict(dataDict, orient=\"index\", columns=[\"r\"+str(i) for i in list(range(1,65))])\n",
    "        resultsDF.index = pd.MultiIndex.from_tuples(resultsDF.index, names=('alpha', 'item',\"som_variant\",\"exponent\",\"display_mode\",\"use_triangular\",\"use_prob_candidate_selection\"))\n",
    "        resultsDF = resultsDF.reset_index()                        \n",
    "        #resultsDF[list(range(1,65))] = pd.DataFrame(resultsDF.results.tolist(), index=resultsDF.index)\n",
    "        #resultsDF.drop(\"results\",axis=1, inplace=True)\n",
    "        resultsDF[\"mean\"] = resultsDF[[\"r\"+str(i) for i in list(range(1,65))]].mean(axis=1)\n",
    "        #print(resultsDF.groupby(\"som_variant\")[\"mean\"].mean())\n",
    "        \n",
    "        return resultsDF       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute evaluation\n",
    "- skip next cell and load stored results if you do not want to re-calculate the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.001 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.003 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.01 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.03 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.1 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.2 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.5 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.8 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.9 exponent:5\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.001 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.003 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.01 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.03 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.1 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.2 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.5 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.8 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.9 exponent:20\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.001 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.003 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.01 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.03 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.1 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.2 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.5 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.8 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n",
      "alpha: 0.9 exponent:10\n",
      "test-case: 0\n",
      "test-case: 10\n",
      "test-case: 20\n",
      "test-case: 30\n",
      "test-case: 40\n",
      "test-case: 50\n",
      "test-case: 60\n",
      "test-case: 70\n",
      "test-case: 80\n",
      "test-case: 90\n"
     ]
    }
   ],
   "source": [
    "evalProc = EvalProcess([\"rating\",\"topk\",\"normal\"], #,\"rank\",\"rank_pos\"\n",
    "                       [0.001, 0.003, 0.01, 0.03, 0.1, 0.2, 0.5, 0.8, 0.9], #0.0001,0.0003,\n",
    "                       \"topk\", \n",
    "                       [5,20,10], \n",
    "                       500, \n",
    "                       100, \n",
    "                       0.1, \n",
    "                       0.1,\n",
    "                       True)\n",
    "(resultingIDX,ratings,ratingsUnbiased) = evalProc.runConfigurations()\n",
    "import pickle\n",
    "ratingsDF = processData(ratings)\n",
    "ratingsUnbDF = processData(ratingsUnbiased)\n",
    "resultingIDXDF = processData(resultingIDX)\n",
    "\n",
    "pickle.dump(evalProc,open(\"pck/final/evalProcEXP3\"+\"_disp\"+str(evalProc.display_mode)+\"_triag\"+str(evalProc.use_triangular)+\".pck\", \"wb\"))\n",
    "ratingsDF.to_pickle(\"pck/final/ratingsDFEXP3\"+\"_disp\"+str(evalProc.display_mode)+\"_triag\"+str(evalProc.use_triangular)+\".pck\")\n",
    "ratingsUnbDF.to_pickle(\"pck/final/ratingsUnbDFEXP3\"+\"_disp\"+str(evalProc.display_mode)+\"_triag\"+str(evalProc.use_triangular)+\".pck\")\n",
    "resultingIDXDF.to_pickle(\"pck/final/resultingIDXDFEXP3\"+\"_disp\"+str(evalProc.display_mode)+\"_triag\"+str(evalProc.use_triangular)+\".pck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "evalProc = pickle.load(open(\"pck/final/evalProcEXP3_disptopk_triagTrue.pck\", \"rb\"))\n",
    "ratingsDF = pickle.load(open(\"pck/final/ratingsDFEXP3_disptopk_triagTrue.pck\", \"rb\"))\n",
    "ratingsUnbDF = pickle.load(open(\"pck/final/ratingsUnbDFEXP3_disptopk_triagTrue.pck\", \"rb\"))\n",
    "resultingIDXDF = pickle.load(open(\"pck/final/resultingIDXDFEXP3_disptopk_triagTrue.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "som_variant  exponent\n",
       "normal       5           0.201827\n",
       "             10          0.056337\n",
       "             20          0.015987\n",
       "rating       5           0.230903\n",
       "             10          0.068145\n",
       "             20          0.015978\n",
       "topk         5           0.262094\n",
       "             10          0.086196\n",
       "             20          0.016140\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsDF.groupby([\"som_variant\",\"exponent\"])[\"mean\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "som_variant  exponent\n",
       "normal       5           0.199377\n",
       "             10          0.054110\n",
       "             20          0.016551\n",
       "rating       5           0.225664\n",
       "             10          0.065648\n",
       "             20          0.016693\n",
       "topk         5           0.254855\n",
       "             10          0.084962\n",
       "             20          0.017146\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsUnbDF.groupby([\"som_variant\",\"exponent\"])[\"mean\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateNDCG(idsDisplayed, ratingsAll, use_triangular, SOM_dim):\n",
    "    DCGPenalty = 1/np.log2(np.array(range(SOM_dim**2))+2)\n",
    "    if use_triangular:\n",
    "        dcgOrdering = getGoldenTriangleOrdering(SOM_dim).reshape(-1)\n",
    "        DCGPenalty = DCGPenalty[dcgOrdering]\n",
    "    positions = [\"r\"+str(i) for i in list(range(1,SOM_dim**2+1))]    \n",
    "    \n",
    "    dataIdeal = ratingsAll[:,idsDisplayed.item].T\n",
    "    topkIDX = np.argsort(-dataIdeal, axis=1)[:,0:SOM_dim**2]\n",
    "    #print(topkIDX[0,0:10])\n",
    "    \n",
    "    \n",
    "    if use_triangular:\n",
    "        topkIDX = topkIDX[:,dcgOrdering] \n",
    "    #print(topkIDX[0,0:10])\n",
    "    #print(DCGPenalty[0:10])\n",
    "        \n",
    "    bestRatings = []\n",
    "    for i in range(topkIDX.shape[0]):\n",
    "        bestRatings.append(np.take(dataIdeal[i], topkIDX[i]))\n",
    "    bestRatingsArr = np.stack(bestRatings) \n",
    "    \n",
    "    \n",
    "    ids = idsDisplayed[positions].to_numpy()\n",
    "    print(ids.shape,ratingsAll.shape)\n",
    "    receivedRatings = []\n",
    "    for i in range(topkIDX.shape[0]):\n",
    "        receivedRatings.append(np.take(dataIdeal[i], ids[i]))\n",
    "    receivedRatings = np.stack(receivedRatings)     \n",
    "    \n",
    "    \n",
    "    print(bestRatingsArr[0,0:10])\n",
    "    print(receivedRatings[0,0:10])\n",
    "    \n",
    "    iDCG = np.sum(bestRatingsArr*DCGPenalty, axis=1)\n",
    "    DCG = np.sum(receivedRatings*DCGPenalty, axis=1)\n",
    "    nDCG = DCG/iDCG\n",
    "    \n",
    "    return nDCG\n",
    "\n",
    "def calculateOverallDiversity(displayedIDX, distances, SOM_dim):\n",
    "    positions = [\"r\"+str(i) for i in list(range(1,SOM_dim**2+1))]    \n",
    "    \n",
    "    data = displayedIDX[positions]\n",
    "    df2 = pd.DataFrame(index=data.index)\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(i+1,data.shape[1]):\n",
    "            df2[\"d_\"+str(i+1)+\"_\"+str(j+1)] = distances[data[\"r\"+str(i+1)],data[\"r\"+str(j+1)]]\n",
    "    return df2.mean(axis=1),df2.max(axis=1) \n",
    "\n",
    "def neighbors(i, SOM_dim):\n",
    "    minVal = 0\n",
    "    maxVal = SOM_dim - 1\n",
    "    vals = []\n",
    "    ii = i // SOM_dim\n",
    "    ij = i % SOM_dim\n",
    "    for a in [-1,0,1]:\n",
    "        for b in [-1,0,1]:\n",
    "            if ii+a >= minVal and ij+b >= minVal and ii+a <= maxVal and ij+b <= maxVal:\n",
    "                val = (ii+a)*SOM_dim + ij+b\n",
    "                if val != i:\n",
    "                    vals.append(val) \n",
    "    return vals\n",
    "            \n",
    "\n",
    "def calculateNeighborsDiversity(displayedIDX, distances, SOM_dim):\n",
    "    positions = [\"r\"+str(i) for i in list(range(1,SOM_dim**2+1))]    \n",
    "    \n",
    "    data = displayedIDX[positions]\n",
    "    df2 = pd.DataFrame(index=data.index)\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in neighbors(i,SOM_dim):\n",
    "            #print(i,j)\n",
    "            df2[\"d_\"+str(i+1)+\"_\"+str(j+1)] = distances[data[\"r\"+str(i+1)],data[\"r\"+str(j+1)]]\n",
    "    return df2.mean(axis=1),df2.max(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-a6c3e94c78d2>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"d_\"+str(i+1)+\"_\"+str(j+1)] = distances[data[\"r\"+str(i+1)],data[\"r\"+str(j+1)]]\n"
     ]
    }
   ],
   "source": [
    "meanDistN, maxDistN = calculateNeighborsDiversity(resultingIDXDF, evalProc.pairwiseSampleDistances, evalProc.SOM_dim)\n",
    "resultingIDXDF[\"meanNeighborDiversity\"]  =  meanDistN \n",
    "resultingIDXDF[\"maxNeighborDiversity\"]  =  maxDistN \n",
    "    \n",
    "#resultingIDXDF.groupby([\"alpha\",\"som_variant\"])[\"meanNeighborDiversity\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-a6c3e94c78d2>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"d_\"+str(i+1)+\"_\"+str(j+1)] = distances[data[\"r\"+str(i+1)],data[\"r\"+str(j+1)]]\n"
     ]
    }
   ],
   "source": [
    "meanDist, maxDist = calculateOverallDiversity(resultingIDXDF, evalProc.pairwiseSampleDistances, evalProc.SOM_dim)\n",
    "resultingIDXDF[\"meanOverallDiversity\"]  =  meanDist \n",
    "resultingIDXDF[\"maxDiversity\"]  =  maxDist \n",
    "    \n",
    "#resultingIDXDF.groupby([\"alpha\",\"som_variant\"])[\"meanOverallDiversity\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400, 64) (500, 100)\n",
      "[0.45200722 0.33682522 0.24239156 0.19719773 0.1435492  0.11353143\n",
      " 0.09628046 0.07285793 0.28562875 0.2141814 ]\n",
      "[0.33682522 0.24239156 0.20782134 0.15661693 0.12638552 0.04951054\n",
      " 0.2141814  0.18648199 0.1435492  0.19719773]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>item</th>\n",
       "      <th>som_variant</th>\n",
       "      <th>exponent</th>\n",
       "      <th>display_mode</th>\n",
       "      <th>use_triangular</th>\n",
       "      <th>use_prob_candidate_selection</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>...</th>\n",
       "      <th>r62</th>\n",
       "      <th>r63</th>\n",
       "      <th>r64</th>\n",
       "      <th>mean</th>\n",
       "      <th>meanNeighborDiversity</th>\n",
       "      <th>maxNeighborDiversity</th>\n",
       "      <th>meanOverallDiversity</th>\n",
       "      <th>maxDiversity</th>\n",
       "      <th>nDCG</th>\n",
       "      <th>neighborOverallRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>rating</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>277</td>\n",
       "      <td>148</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>419</td>\n",
       "      <td>117</td>\n",
       "      <td>101</td>\n",
       "      <td>232.843750</td>\n",
       "      <td>0.297194</td>\n",
       "      <td>0.668026</td>\n",
       "      <td>0.367378</td>\n",
       "      <td>0.834677</td>\n",
       "      <td>0.870601</td>\n",
       "      <td>0.808960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>topk</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>277</td>\n",
       "      <td>148</td>\n",
       "      <td>434</td>\n",
       "      <td>...</td>\n",
       "      <td>494</td>\n",
       "      <td>146</td>\n",
       "      <td>130</td>\n",
       "      <td>242.531250</td>\n",
       "      <td>0.343049</td>\n",
       "      <td>0.668026</td>\n",
       "      <td>0.355775</td>\n",
       "      <td>0.668026</td>\n",
       "      <td>0.888046</td>\n",
       "      <td>0.964229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>141</td>\n",
       "      <td>478</td>\n",
       "      <td>289</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>102</td>\n",
       "      <td>37</td>\n",
       "      <td>233.109375</td>\n",
       "      <td>0.208589</td>\n",
       "      <td>0.407277</td>\n",
       "      <td>0.435456</td>\n",
       "      <td>0.893042</td>\n",
       "      <td>0.454763</td>\n",
       "      <td>0.479012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>rating</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>145</td>\n",
       "      <td>100</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>486</td>\n",
       "      <td>25</td>\n",
       "      <td>173</td>\n",
       "      <td>237.406250</td>\n",
       "      <td>0.249580</td>\n",
       "      <td>0.589724</td>\n",
       "      <td>0.322604</td>\n",
       "      <td>0.707862</td>\n",
       "      <td>0.690913</td>\n",
       "      <td>0.773643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>topk</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>145</td>\n",
       "      <td>131</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>310</td>\n",
       "      <td>132</td>\n",
       "      <td>245.437500</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.582627</td>\n",
       "      <td>0.314976</td>\n",
       "      <td>0.595898</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.954360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>0.900</td>\n",
       "      <td>97</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>279</td>\n",
       "      <td>313</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>200</td>\n",
       "      <td>17</td>\n",
       "      <td>238.421875</td>\n",
       "      <td>0.229264</td>\n",
       "      <td>0.766835</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>0.818340</td>\n",
       "      <td>0.612750</td>\n",
       "      <td>0.580389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>0.900</td>\n",
       "      <td>98</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>368</td>\n",
       "      <td>425</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>343</td>\n",
       "      <td>286</td>\n",
       "      <td>222.062500</td>\n",
       "      <td>0.273088</td>\n",
       "      <td>0.619397</td>\n",
       "      <td>0.565203</td>\n",
       "      <td>1.127429</td>\n",
       "      <td>0.154573</td>\n",
       "      <td>0.483168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>0.900</td>\n",
       "      <td>98</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>156</td>\n",
       "      <td>52</td>\n",
       "      <td>447</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>44</td>\n",
       "      <td>290</td>\n",
       "      <td>241.984375</td>\n",
       "      <td>0.183749</td>\n",
       "      <td>0.443501</td>\n",
       "      <td>0.325865</td>\n",
       "      <td>0.719370</td>\n",
       "      <td>0.881647</td>\n",
       "      <td>0.563880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>0.900</td>\n",
       "      <td>99</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>65</td>\n",
       "      <td>268</td>\n",
       "      <td>238</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>214</td>\n",
       "      <td>445</td>\n",
       "      <td>225.328125</td>\n",
       "      <td>0.272667</td>\n",
       "      <td>0.539640</td>\n",
       "      <td>0.562556</td>\n",
       "      <td>1.151974</td>\n",
       "      <td>0.094424</td>\n",
       "      <td>0.484692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>0.900</td>\n",
       "      <td>99</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>422</td>\n",
       "      <td>380</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>489</td>\n",
       "      <td>445</td>\n",
       "      <td>306</td>\n",
       "      <td>282.359375</td>\n",
       "      <td>0.201752</td>\n",
       "      <td>0.440090</td>\n",
       "      <td>0.365231</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>0.525020</td>\n",
       "      <td>0.552394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4400 rows  78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  item som_variant  exponent display_mode  use_triangular  \\\n",
       "0     0.001     0      rating         5         topk            True   \n",
       "1     0.001     0        topk         5         topk            True   \n",
       "2     0.001     0      normal         5         topk            True   \n",
       "3     0.001     1      rating         5         topk            True   \n",
       "4     0.001     1        topk         5         topk            True   \n",
       "...     ...   ...         ...       ...          ...             ...   \n",
       "4395  0.900    97      rating        10         topk            True   \n",
       "4396  0.900    98      rating        10         topk            True   \n",
       "4397  0.900    98      rating        10         topk            True   \n",
       "4398  0.900    99      rating        10         topk            True   \n",
       "4399  0.900    99      rating        10         topk            True   \n",
       "\n",
       "      use_prob_candidate_selection   r1   r2   r3  ...  r62  r63  r64  \\\n",
       "0                             True  277  148   33  ...  419  117  101   \n",
       "1                             True  277  148  434  ...  494  146  130   \n",
       "2                             True  141  478  289  ...  224  102   37   \n",
       "3                             True  145  100  131  ...  486   25  173   \n",
       "4                             True  145  131  203  ...  390  310  132   \n",
       "...                            ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "4395                          True  279  313  135  ...   45  200   17   \n",
       "4396                         False  368  425  116  ...  132  343  286   \n",
       "4397                          True  156   52  447  ...  157   44  290   \n",
       "4398                         False   65  268  238  ...  239  214  445   \n",
       "4399                          True  422  380  426  ...  489  445  306   \n",
       "\n",
       "            mean  meanNeighborDiversity  maxNeighborDiversity  \\\n",
       "0     232.843750               0.297194              0.668026   \n",
       "1     242.531250               0.343049              0.668026   \n",
       "2     233.109375               0.208589              0.407277   \n",
       "3     237.406250               0.249580              0.589724   \n",
       "4     245.437500               0.300600              0.582627   \n",
       "...          ...                    ...                   ...   \n",
       "4395  238.421875               0.229264              0.766835   \n",
       "4396  222.062500               0.273088              0.619397   \n",
       "4397  241.984375               0.183749              0.443501   \n",
       "4398  225.328125               0.272667              0.539640   \n",
       "4399  282.359375               0.201752              0.440090   \n",
       "\n",
       "      meanOverallDiversity  maxDiversity      nDCG  neighborOverallRatio  \n",
       "0                 0.367378      0.834677  0.870601              0.808960  \n",
       "1                 0.355775      0.668026  0.888046              0.964229  \n",
       "2                 0.435456      0.893042  0.454763              0.479012  \n",
       "3                 0.322604      0.707862  0.690913              0.773643  \n",
       "4                 0.314976      0.595898  0.699887              0.954360  \n",
       "...                    ...           ...       ...                   ...  \n",
       "4395              0.395018      0.818340  0.612750              0.580389  \n",
       "4396              0.565203      1.127429  0.154573              0.483168  \n",
       "4397              0.325865      0.719370  0.881647              0.563880  \n",
       "4398              0.562556      1.151974  0.094424              0.484692  \n",
       "4399              0.365231      0.771693  0.525020              0.552394  \n",
       "\n",
       "[4400 rows x 78 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsUnbDF[\"nDCG\"] = calculateNDCG(resultingIDXDF, evalProc.unbiasedRatings, evalProc.use_triangular, evalProc.SOM_dim)\n",
    "resultingIDXDF[\"nDCG\"] = ratingsUnbDF[\"nDCG\"]\n",
    "resultingIDXDF[\"neighborOverallRatio\"] = resultingIDXDF.meanNeighborDiversity / resultingIDXDF.meanOverallDiversity\n",
    "meanRes = resultingIDXDF.groupby([\"alpha\",\"som_variant\",\"use_prob_candidate_selection\",\"exponent\"])[[\"nDCG\",\"meanOverallDiversity\",\"meanNeighborDiversity\"]].mean()\n",
    "meanRes[\"neighborOverallRatio\"] = meanRes.meanNeighborDiversity / meanRes.meanOverallDiversity\n",
    "resultingIDXDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>item</th>\n",
       "      <th>som_variant</th>\n",
       "      <th>exponent</th>\n",
       "      <th>display_mode</th>\n",
       "      <th>use_triangular</th>\n",
       "      <th>use_prob_candidate_selection</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>...</th>\n",
       "      <th>r62</th>\n",
       "      <th>r63</th>\n",
       "      <th>r64</th>\n",
       "      <th>mean</th>\n",
       "      <th>meanNeighborDiversity</th>\n",
       "      <th>maxNeighborDiversity</th>\n",
       "      <th>meanOverallDiversity</th>\n",
       "      <th>maxDiversity</th>\n",
       "      <th>nDCG</th>\n",
       "      <th>neighborOverallRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>rating</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>277</td>\n",
       "      <td>148</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>419</td>\n",
       "      <td>117</td>\n",
       "      <td>101</td>\n",
       "      <td>232.843750</td>\n",
       "      <td>0.297194</td>\n",
       "      <td>0.668026</td>\n",
       "      <td>0.367378</td>\n",
       "      <td>0.834677</td>\n",
       "      <td>0.870601</td>\n",
       "      <td>0.808960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>141</td>\n",
       "      <td>478</td>\n",
       "      <td>289</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>102</td>\n",
       "      <td>37</td>\n",
       "      <td>233.109375</td>\n",
       "      <td>0.208589</td>\n",
       "      <td>0.407277</td>\n",
       "      <td>0.435456</td>\n",
       "      <td>0.893042</td>\n",
       "      <td>0.454763</td>\n",
       "      <td>0.479012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>rating</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>145</td>\n",
       "      <td>100</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>486</td>\n",
       "      <td>25</td>\n",
       "      <td>173</td>\n",
       "      <td>237.406250</td>\n",
       "      <td>0.249580</td>\n",
       "      <td>0.589724</td>\n",
       "      <td>0.322604</td>\n",
       "      <td>0.707862</td>\n",
       "      <td>0.690913</td>\n",
       "      <td>0.773643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>186</td>\n",
       "      <td>83</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>371</td>\n",
       "      <td>53</td>\n",
       "      <td>390</td>\n",
       "      <td>235.187500</td>\n",
       "      <td>0.207256</td>\n",
       "      <td>0.399076</td>\n",
       "      <td>0.422674</td>\n",
       "      <td>0.949546</td>\n",
       "      <td>0.368993</td>\n",
       "      <td>0.490344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>rating</td>\n",
       "      <td>5</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>83</td>\n",
       "      <td>348</td>\n",
       "      <td>164</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>302</td>\n",
       "      <td>57</td>\n",
       "      <td>254.781250</td>\n",
       "      <td>0.266610</td>\n",
       "      <td>0.574346</td>\n",
       "      <td>0.319786</td>\n",
       "      <td>0.724387</td>\n",
       "      <td>0.706968</td>\n",
       "      <td>0.833716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>0.900</td>\n",
       "      <td>97</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>279</td>\n",
       "      <td>313</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>200</td>\n",
       "      <td>17</td>\n",
       "      <td>238.421875</td>\n",
       "      <td>0.229264</td>\n",
       "      <td>0.766835</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>0.818340</td>\n",
       "      <td>0.612750</td>\n",
       "      <td>0.580389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>0.900</td>\n",
       "      <td>98</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>368</td>\n",
       "      <td>425</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>343</td>\n",
       "      <td>286</td>\n",
       "      <td>222.062500</td>\n",
       "      <td>0.273088</td>\n",
       "      <td>0.619397</td>\n",
       "      <td>0.565203</td>\n",
       "      <td>1.127429</td>\n",
       "      <td>0.154573</td>\n",
       "      <td>0.483168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>0.900</td>\n",
       "      <td>98</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>156</td>\n",
       "      <td>52</td>\n",
       "      <td>447</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>44</td>\n",
       "      <td>290</td>\n",
       "      <td>241.984375</td>\n",
       "      <td>0.183749</td>\n",
       "      <td>0.443501</td>\n",
       "      <td>0.325865</td>\n",
       "      <td>0.719370</td>\n",
       "      <td>0.881647</td>\n",
       "      <td>0.563880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>0.900</td>\n",
       "      <td>99</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>65</td>\n",
       "      <td>268</td>\n",
       "      <td>238</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>214</td>\n",
       "      <td>445</td>\n",
       "      <td>225.328125</td>\n",
       "      <td>0.272667</td>\n",
       "      <td>0.539640</td>\n",
       "      <td>0.562556</td>\n",
       "      <td>1.151974</td>\n",
       "      <td>0.094424</td>\n",
       "      <td>0.484692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>0.900</td>\n",
       "      <td>99</td>\n",
       "      <td>rating</td>\n",
       "      <td>10</td>\n",
       "      <td>topk</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>422</td>\n",
       "      <td>380</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>489</td>\n",
       "      <td>445</td>\n",
       "      <td>306</td>\n",
       "      <td>282.359375</td>\n",
       "      <td>0.201752</td>\n",
       "      <td>0.440090</td>\n",
       "      <td>0.365231</td>\n",
       "      <td>0.771693</td>\n",
       "      <td>0.525020</td>\n",
       "      <td>0.552394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4100 rows  78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  item som_variant  exponent display_mode  use_triangular  \\\n",
       "0     0.001     0      rating         5         topk            True   \n",
       "2     0.001     0      normal         5         topk            True   \n",
       "3     0.001     1      rating         5         topk            True   \n",
       "5     0.001     1      normal         5         topk            True   \n",
       "6     0.001     2      rating         5         topk            True   \n",
       "...     ...   ...         ...       ...          ...             ...   \n",
       "4395  0.900    97      rating        10         topk            True   \n",
       "4396  0.900    98      rating        10         topk            True   \n",
       "4397  0.900    98      rating        10         topk            True   \n",
       "4398  0.900    99      rating        10         topk            True   \n",
       "4399  0.900    99      rating        10         topk            True   \n",
       "\n",
       "      use_prob_candidate_selection   r1   r2   r3  ...  r62  r63  r64  \\\n",
       "0                             True  277  148   33  ...  419  117  101   \n",
       "2                             True  141  478  289  ...  224  102   37   \n",
       "3                             True  145  100  131  ...  486   25  173   \n",
       "5                             True  186   83  117  ...  371   53  390   \n",
       "6                             True   83  348  164  ...  225  302   57   \n",
       "...                            ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "4395                          True  279  313  135  ...   45  200   17   \n",
       "4396                         False  368  425  116  ...  132  343  286   \n",
       "4397                          True  156   52  447  ...  157   44  290   \n",
       "4398                         False   65  268  238  ...  239  214  445   \n",
       "4399                          True  422  380  426  ...  489  445  306   \n",
       "\n",
       "            mean  meanNeighborDiversity  maxNeighborDiversity  \\\n",
       "0     232.843750               0.297194              0.668026   \n",
       "2     233.109375               0.208589              0.407277   \n",
       "3     237.406250               0.249580              0.589724   \n",
       "5     235.187500               0.207256              0.399076   \n",
       "6     254.781250               0.266610              0.574346   \n",
       "...          ...                    ...                   ...   \n",
       "4395  238.421875               0.229264              0.766835   \n",
       "4396  222.062500               0.273088              0.619397   \n",
       "4397  241.984375               0.183749              0.443501   \n",
       "4398  225.328125               0.272667              0.539640   \n",
       "4399  282.359375               0.201752              0.440090   \n",
       "\n",
       "      meanOverallDiversity  maxDiversity      nDCG  neighborOverallRatio  \n",
       "0                 0.367378      0.834677  0.870601              0.808960  \n",
       "2                 0.435456      0.893042  0.454763              0.479012  \n",
       "3                 0.322604      0.707862  0.690913              0.773643  \n",
       "5                 0.422674      0.949546  0.368993              0.490344  \n",
       "6                 0.319786      0.724387  0.706968              0.833716  \n",
       "...                    ...           ...       ...                   ...  \n",
       "4395              0.395018      0.818340  0.612750              0.580389  \n",
       "4396              0.565203      1.127429  0.154573              0.483168  \n",
       "4397              0.325865      0.719370  0.881647              0.563880  \n",
       "4398              0.562556      1.151974  0.094424              0.484692  \n",
       "4399              0.365231      0.771693  0.525020              0.552394  \n",
       "\n",
       "[4100 rows x 78 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultingIDXDFReduced = resultingIDXDF.loc[resultingIDXDF.som_variant.isin([\"normal\",\"topk\",\"rating\"])]\n",
    "resultingIDXDFReduced = resultingIDXDFReduced.drop(resultingIDXDFReduced.loc[(resultingIDXDFReduced.som_variant==\"topk\") & (resultingIDXDFReduced.use_prob_candidate_selection==True)].index)\n",
    "resultingIDXDFReduced = resultingIDXDFReduced.drop(resultingIDXDFReduced.loc[(resultingIDXDFReduced.alpha.isin([0.003, 0.01,0.03,0.1,0.2,0.5,0.8,0.9,0.93])) & (resultingIDXDFReduced.som_variant.isin([\"normal\",\"topk\"]))].index)\n",
    "resultingIDXDFReduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "meanResReduced = meanRes.loc[idx[:, [\"normal\",\"topk\",\"rating\"],:,:], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanResReduced = meanResReduced.drop(meanResReduced.loc[idx[:, \"topk\",True,:],:].index)\n",
    "meanResReduced = meanResReduced.drop(meanResReduced.loc[idx[:,:,:, 20],:].index)\n",
    "#meanResReduced = meanResReduced.drop(meanResReduced.loc[idx[[0.0001, 0.0003], [\"rating\"],:,:],:].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Index([0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.003, 0.003,\n",
       "              0.003,  0.01,  0.01,  0.01,  0.03,  0.03,  0.03,   0.1,   0.1,\n",
       "                0.1,   0.2,   0.2,   0.2,   0.5,   0.5,   0.5,   0.8,   0.8,\n",
       "                0.8,   0.9,   0.9,   0.9],\n",
       "             dtype='float64', name='alpha')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanResReduced.index.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEYCAYAAAAeWvJ8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABU3ElEQVR4nO3deXgUVdbA4d/JRhI2BQFRVBZBdiJkcHBBEHFBwF1QcMi4DSoqKC444sIMiqKOjjIqisr4yeIOKAjIoqOjQoAggjAgogKCEPZA9vP9UdVNZ68knXQnOe/z1JPu6qrbt0PIya176xxRVYwxxphQiQh1B4wxxtRsFoiMMcaElAUiY4wxIWWByBhjTEhZIDLGGBNSUaHuQLAcd9xx2rx581B3wxhj/FasWLFbVRsFo60uZ7bTg/vSPB370w+/zlfVi4LxvpWh2gSi5s2bk5ycHOpuGGOMn4j8HKy2Du5L429v3ePp2KGJI48L1vtWhmoTiIwxprpTJNRdqBAWiIwxpgpQoLrmH7BAZIwxVURuqDtQQSwQGWNMVaA2IjLGGBNC6m7VkQUiY4ypImxEZIwxJqQsEBljjAmpahqHLBAZY0xVYSMiY4wxIaNArgUiY4wxoWT3ERljjAkdu4/IGGNMKNl9RMYYY0LORkTGGGNCqprGIQtERTmSnkl2Vg5R0ZHExcaEujvGGFNtR0RWKjyfI0cySM/I5Lvvfua8Cx/ju+9+5kh6Zqi7ZYwx/nmikraqxgKR6+DBIxw8eITZH69g7540nnjqQ9LTsxg+YrIFI2NMyPnqEXnZqhoLRMCBA4eZPvMrduzcx2eLvuPWO15lwvghND+lEdnZuRaMjDFhwUZE1dSBA4d5/c3FvPCvedz0l5e5c0Q/Gh1Xjwf++jYTJ1xvwcgYEzZsRFQGInKRiGwQkU0i8kAhryeJyC4RSXG3mwJeywnYP7si+ucLQlPeXALAnr2HGHHXlCKDUfKKH8nKzK6IrhhjTLFUIcfjVtVU2Ko5EYkEJgF9ga3AchGZrarr8h06U1VHFNLEEVVNqKj+5Q9CPr5g9OLzN/LPF+fy14enM/lff+H9D7/h+iHnUq9eXEV1yRhjilUVRzteVOTy7e7AJlXdDCAiM4BLgfyBqNLtP3CYxUvWFAhCPnv2HmLkPW/w/szRLF/xIxv+t50/De1lQcgYE1LVNA5V6KW5E4FfA55vdffld6WIfCci74nISQH7Y0UkWUS+EZHLCnsDEbnFPSZ5165dJXZIVdn+215G3j+VU09tyhndWxd6XFRUJA/efwU//7qb6KhIup7e0oKQMSbkbI6oYswBmqtqZ2AhMDXgtVNUNRG4DnhORFrlP1lVJ6tqoqomNmrUqMQ327svjT/dPInkFZsZcffr3HVHvwLBKCoqkqcnXM+JJzYgLjbagpAxJmwEc9Wchzn8U0RkkTtQWCoizYLzKQqqyEC0DQgc4TRz9/mpaqqqZrhPXwO6Bby2zf26GVgKnF6ezqgqX3z5Azt/3w/Anr1pBYJRVFQkz078Eyee2IBTTm5Eq5bHWxAyxoSFYN5HFDCHfzHQHrhWRNrnO+xp4N/uQGEc8ERQP1CAigxEy4HWItJCRGKAwUCe1W8i0jTg6UDgB3f/sSJSy318HHAW5ZxbOnDwCLPnrsizLzAYnX1mW56dOIzTE1pwysmNiIuLITIy1ANGY4w5KogjIv8cvqpmAr45/EDtgcXu4yWFvF6AiNQRkTreunBUhf2mVdVsYAQwHyfAvKOqa0VknIgMdA+7U0TWishq4E4gyd3fDkh29y8BJhSy2q60/SErK6fAfl8wumPExSxbsYnv124lLq5m5ZZTVX7buY/fd+0PdVeMMcXIVW8bcJxv/tzdbsnXlJc5/NXAFe7jy4G6ItKwsH6JSCcRWQWsBdaJyAoR6ej1c1Vo0lNVnQvMzbfv4YDHY4AxhZz3X6BTMPtSOz6WP3ZvTcp3Wwq8tmdvGoP+9DwiwrAh5wbzbcOeqrLj9/1cf8skoqMimfryrTRuVD/U3TLGFKIU6xB2u3Ps5TEaeFFEkoAvcKZWCv4173gFuFtVlwCISC9gMnCmlzeqMdeeoqMjueqyM4iOjizymHPOakt8DRoNBQahHTv38eu2VIYNf8lGRsaEIa/zQx5XzXmZw9+uqleo6unAX919+4por7YvCLnHLQVqe/1sNSYQAdSvF89Lz91ETEzBgWDbNicw7qGrqVcvPgQ9q3z5g5CPBSNjwlcQ54i8zOEfJyK+GDEGeL2Y9jaLyFgRae5uDwGbvX6uGlWPKDY2moTOpzB/1oN8PG8lySt/JDY2hmuu+COtT23Kscd4DuBV3s5dBYOQjy8YTX3lNhofV6/yO2eMKVSw7hFS1WwR8c3hRwKv++bwgWRVnQ30Ap4QEcW5NHd7MU3eADwGfOA+/4+7z5MaFYgAatWKplataK6/9hyuvvwMIiMjiK2Bhe+ioyI5pn58oYEIoMGxdYiyVYPGhJXcIN6s6mEO/z3gPY9t7cVZcFYmNfY3TWRkBLVrx9bIIATQsEFdXnnuJtq2OaHAa106nsILE5NocGypV2EaYyqI18tylZlYQUSec7/OEZHZ+Tev7dS4EZE5yheM/jLyNdb/bztgQciYcBbMEVGQvOV+fbo8jdTYEZFxBI6MLAgZE97CbUSkqr4sAQmq+nngBiR4bcdGRMYfjETEgpAxYSyME5oOA57Pty+pkH2FskBkACcYGWPCV2WPdrwQkWtxElO3yDcnVBfY47UdC0SmyjiYlg6q1K1jiWhNDaTO/X9h5r/Ab8BxwDMB+w8C33ltxAKRqRIOpaUzb9FqFiz9nqcfGcwx9WvOPV/G+IRbGFLVn4GfgR7laccWK5iwdygtnbmLVvPY0x/xdfImRj82g33700LdLWMqXbgWxhORP4rIchE5JCKZIpIjIge8nm+ByIS1wCDkY8HI1FThGoiAF4FrgY1AHHATTr0jTywQmbBVWBDysWBkappwvKE1T/9UNwGRqpqjqm8AF3k91wKRCVu5ucqLUz4r8vWvkzex7n/byc4pKjO9MdVLKeoRVbbDbvLUFBF5SkRGUYr4YoHIhK06tWvx9r+G06CIZLQTHrqGzu1PIiqy6NIexlQnYXxp7nqceDICSMMpMXFFsWcEsEBkwlZERAQnNj2WaS/dWiAYTXjoGnqf1Y46tWND1DtjKl+4XppT1Z9VNV1VD6jqY8DfcEpLeGKByIS1woKRBSFTEwW5MF5QiMhJIjJZRD4WkZtEpLaIPANsABp7bcfuIzJhLzAYrd2wjbPPaGNByNRI4XYfEfBv4HPgfZzFCclACtBZVXd4bcQCkakSfMGoYYM6xMfVCnV3jAmJ8EusQANVfdR9PF9ErgaGqGpuaRqxQGSqjIiICAtCpkYLvzgEInIsIO7TVKC+iAiAqnrKN2eByBhjqogwHBHVB1ZwNBABrHS/KtDSSyMWiIwxpgpQwi8QqWrzYLRjgcgYY6qIMItDQWOByBhjqohwGxEFiwUiY4ypCkKXvqfC2Q2txhhTBYRz0lMReUZEOpT1fAtExhhTVYRrJIIfgMki8q2IDBeR+qU52QKRMcZUEeGW4udov/Q1VT0L+BPQHPhORKaJSG8v51sgMsaYKiJ8B0QgIpFAW3fbDawG7haRGSWda4sVjDGmigjXVXMi8g+gP7AYeFxVl7kvPSkiG0o6v0JHRCJykYhsEJFNIvJAIa8nicguEUlxt5sCXhsmIhvdbVhF9tMYY6qCMB4RfQckqOpfAoKQT/eSTq6wQOQO0yYBFwPtgWtFpH0hh85U1QR3e809twHwCHAGzod4xM1nZIwxNZIvs0I4zhEBQ1U1LXCHiCwCUNX9JZ1ckSOi7sAmVd2sqpnADOBSj+deCCxU1T2quhdYSCnqnxtjTHUUbiMiEYl1Bw7HicixItLA3ZoDJ3ptpyLniE4Efg14vhVnhJPflSLSE/gfMEpVfy3i3AIfSkRuAW4BOPnkk4PUbWOMCUOhG+0U5y/ASOAEjiY7BTgAvOi1kVCvmpsDNFfVzjijnqmlOVlVJ6tqoqomNmrUqEI6aIwxYSPMhkSq+ryqtgBGq2qLgK2LqnoORBU5ItoGnBTwvJm7z09VUwOevgY8FXBur3znLg16D40xpgoJtwGRiJynqouBbSJyRf7XVfUDL+1UZCBaDrQWkRY4gWUwcF3gASLSVFV/c58OxLk7F2A+8HjAAoULgDEV2FdjjAlrSljmmjsXZ8n2gEJeUyC0gUhVs0VkBE5QiQReV9W1IjIOSFbV2cCdIjIQyAb2AEnuuXtE5G84wQxgnNdKf8YYU12F2xyRqj7ifv1zedqp0BtaVXUuMDffvocDHo+hiJGOqr4OvF6R/TPGGFN+InIX8AZwEHgV6Ao8oKoLvJwf6sUKxhhjPArj+4huUNUDONMoDYHrgQleT7YUP8YYUxWE5/JtH3G/9gP+7U7DSHEnBLIRkTHGVBFhtno70AoRWYATiOaLSF0g1+vJNiIyxpgqwJfiJ9y4I5+HgUbAZlU9LCINAc8LGGxEZIwxNZCHpNQni8gSEVklIt+JSL/C2lFVBeaq6kpV3efuS1XV77z2xQKRMcZUEcFarOAxKfVDwDuqejrOfaD/KqbJlSLyhzJ9KOzSnDHGVBlBvDTnT0oN4BavuxRYF/h2QD33cX1gezHtnQEMFZEtQBrO4gV107eVqMRAJCIN86XiMcYYEwKliEPHiUhywPPJqjo54LmXpNSPAgtE5A6gNnB+Me93ofeuFeTl0tw3IvKuiPQrzXI8Y4wxweP1spw7atrtSwjtbpNLaL4w1wJvqmoznNVwb4lIoTFDVX/GyS16nvv4MKWY+vFyYBtgMs4NShtF5HERaeP1DYwxxgRHEG9oLTEpNXAj8I7zvvo1EAscV1hjIvIIcD9HM+VEA//n9XOVGIjUsVBVrwVuBoYBy0TkcxHp4fWNjDHGhA1/UmoRicFZjDA73zG/AH0ARKQdTiDaVUR7l+Mkrk4DUNXtQF2vnfE0RwQMxRkR7QTucDucALwLtPD6ZsYYY8ouWIsVPCalvgd4VURG4UxPJblLtQuTqaoqIgogIrVL0x8vq+a+Bt4CLlPVrQH7k0Xk5dK8mTGm4hxOzyQ+NibU3TAVKJg3tHpISr0OOMtjc++IyCvAMSJyM3ADTvJTT7zMET2kqn8LDEIicrXb0Se9vpExpuLsSD3A3C/XciAtPdRdMRUoXFP8qOrTwHvA+8BpwMOq+oLX872MiB7AnbAKMAbnspwxJsR2pB7gujFv8uvOfRw6nMEVfRKoVzs21N0yQRe+WU9F5G5gpqouLMv5RQYiEbkYZ8neiSLyz4CX6uEUsjPGhFhgEAIYP8Up/2LBqPoJYUJTL+ri3HO0B5gJvKuqO72eXNylue1AMpAOrAjYZlPOm5eMMeWXPwj5jJ+ygA8WpdhluuqmdPcRVW7XVB9T1Q7A7UBT4HMR+czr+UWOiFR1NbBaRN5WVRsBGRNG0o5kMOTBqQWCkM/4KQtoUL82F/yxLbG1oiu3c6bihPGQyPU7sANIBRp7PanIEZGI+OaFfJlXfdsaEfGcVdUYUzHuHdaHonKdtG95PGd1aWlBqJoJ18UKInKbiCwFFuFUaL3Za545KH6xwl3u1/5l754xpiLUjqvFOae34p/3XcWdT72X53JM+5bH8/ojQ2h4TKlu5TBVQJiuVQAnS8NIVU0py8lFjohU9Tf34W7gVzd/UC2gC8VnYTXGVILAYOQbGVkQqt7CbY5IRHzZuScCv4hIg8DNazte7iP6AogVkROBBTgZFt4sbYeNMcEXGIw6ntrUglA15qvQGk6BCJjmfl2Bs7gtcGFbclEn5eflPiJxS7/eCPxLVZ8SkZRSdtYYU0F8wejshFbUia8V6u6YGkRV+7tfy5XqzVMgcpObDsHJxgpObiJjTJioHWcBqNoL0/tZRSQKp9JrW3fXOmB+aVZbe7k0dxdOJoUP3aR4LYElpe2sMcaY8gm3VXPulM1anASpJ+AU3LsPWCsiJ3htp9gRkVvXfKCqDvTtc0vL3lmWThtjjCmH8BsRjQdeUtXnAneKyJ3AEzhlg0pUbCBS1RwRObusPTTGGBM8YXhp7o+qmpR/p6r+U0Q2eG3EyxzRKhGZjZPkNC3gjT7w+ibGGGPKL/ziEEeKee2w10a8BKJYnHQN5wXsU8ACkTHGVKbwi0T1ReSKQvYLToJsT0oMRKr659L0yhhjTPCFafbtz4EBRbz2hddGvJQKbwO8BDRR1Y4i0hlnAcPfvb6JMcaYIAizSaJgDVS8LN9+FWf5dpb7xt8Bg4Px5sYYYzxSyPW4VTVeAlG8qi7Lt8/TjUoicpGIbBCRTSLyQDHHXSkiKiKJ7vPmInJERFLc7WUv72eMMdVauN1IFCReFivsFpFWuB9PRK4Cfiv+FP89SJOAvsBWYLmIzFbVdfmOq4tz0+y3+Zr4UVUTPPTPGGNqhCoYYzzxEohuByYDbUVkG/ATTrqfknQHNrk3wCIiM4BLcdI/BPob8CRwr9dOG2NMTRRmU0QUsWLOz+ttPl4C0c+qer6I1AYiVPWgl4ZxUj38GvB8K3BG4AEi0hU4SVU/EZH8gaiFiKwCDgAPqep/8r+BiNwC3AJw8skne+yWMcZUUWEWiCh6xRyU4jYfL4HoJxH5FJgJLPbSqBciEgE8CyQV8vJvwMmqmioi3YCPRKSDqh4IPEhVJ+OM1khMTAy/fyJjjAmScJz+CdaqOS+BqC1OldbbgSki8jEwQ1W/LOG8bThV+3yauft86gIdgaXiVPU6HpgtIgNVNRnIAFDVFSLyI9CGUtS3MMaY6iYML83dXdzrqvqsl3ZKXDWnqodV9R1VvQI4Hedu2c89tL0caC0iLUQkBmfJ9+yAdver6nGq2lxVmwPf4NyflCwijdzFDrjZvlsDm718IGOMMZWmbgmbJ15GRIjIucAg4CKcUck1JZ2jqtkiMgKYj1O/6HW3jMQ4IFlVZxdzek9gnIhkAbnAcFXd46WvxhhTLYVhPSJVfSwY7XjJrLAFWAW8A9yrqmnFn3GUqs4F5ubb93ARx/YKePw+8L7X9zHGmJogzOKQn4jE4hRO7YCTnxQAVb3By/leRkSd8y8SMMYYEwLhGongLWA9cCEwDucWnx+8nlxkIBKR+1T1KeDv7mKCPFTViuMZY0wlCrdLcwFOVdWrReRSVZ0qItOAArfcFKW4EZEvmq0oV/eMMcaUmwIavpEoy/26T0Q6AjuAxl5PLjIQqeoc9+vUcnXPGGNMUIRtGILJInIs8BDO6ug6wFivJxe7fFtEhonIShFJc7dkEflT+fprjDGmTMIw6ambnOCAqu5V1S9UtaWqNlbVV7y2UWQgEpFhwEjgHuAEnJQ99wF3icj15eu6Md5kZeVw4OCREvd5lZ6eGYxuGVP53OXbXrZK7ZZqLk5sKLPiRkS3Aper6hL35tN9qroYuBIny4IxFSorK4effv6d6e98xYEDh4vc51V2dg5bt+3h4KGyBbHC+5hNbm5u0NozpjhhOCDy+UxERovISSLSwLd5Pbm4xQr1VHVL/p2qukVEPNciN6YsfAHnTze+SFpaBpmZWfz5T73Z/tte/76MjCyShp5LvXrxntrcf+AwScP/xVuvjqBunbhy9/HgoSO8PnUJ11/XkwbH1il3e8aUKHwniQa5XwMHKQq09HJycYGouD8bg/cnpTGFyM7JYe/eQ2SkO4txJk9ZxMZNO1iWvIm0tAwAtm5L9XwZ4siRTGa89xX79x9m3BPv8fzEJM8BrCipqYeY8u8lNGlcnysvO4PoaE+JSowps3CNQ6raojznF3dprp2IfFfItgYnEaoxFSYuNobOHU/hpRduJirS+TFd8vlafxC6+MIE/nrf5dSvX3wwycrKJnXPQV6espBX33CSxyev2swj49/ltx17OXw4o9R9y87JZc/eQzwy/h1UlX++NI+duw5w+Ejp26ppMrKySc/MIrcq1rMOMWf5dvjNEQGISLyIPCQik93nrUWkv9fziw1EOLUm8m/9gfZl77Ix3sTFxZDQ+RSS/tQrz/7jGtZl3NhB1K9fu9jzMzKy+G3HPi4bNJHX/72EnJyjczmfLVlDv8uf4NvkTaSnZxXTSl579x3ik09XctmgiaxM+QmAg4fSGXDVk7z06gJSUw+SmZXt/UPWIBlZ2azb+ju9H53C1tT9FozKInwnid4AMoEz3efbgL97PbnIQKSqPxe3la/PxpQsKyuHX7amMv2dr/Ls3516kFff+KzExQq1akVzXMO6zPz3KPqe1ynPa+3bNWP61LvomtCC2NhoT/05ciSTrKwcFi1dw959eVMuZmfn8J//rue3nftsZV4hfEHommdmsDX1AJc++X8WjMogfOMQrdxMPFngVG0ACqbkKUKJZSCMCYX8ixUA2rRu6r9MN3nKIqa+/XmJwSg+vhYnND2WcQ8N4poregBwWusTmPzPW2jb5kTql2KeKC4uhsaN6vP3sYN47+27adK4PgAREcJT44fyxsu30bH9SdSrW765p+omMAilu6PF3QcPWzAqg2BemhORi0Rkg4hsEpEHCnn9HyKS4m7/E5F9xTSXKSJxuHFQRFrh1pTzwgKRCUv5FytcfGECU1+7Pc+c0a9bvS9WqFMnluE39SUuNoZH/3p1iXNLxalXL542pzbl0b9eDcCVl55BzzPbcuwxxV8qrIkKC0I+FoxKKYj3Ebn13iYBF+NMtVwrInmmXFR1lKomqGoC8ALFl/1+FPgUOElE3gYWUYp7iySMcxeVSmJioiYnOwVcs7Ky2Lp1K+np6SHulSkPVSUzM5vDRzKpXy+OiIiIQveVtr3o6MhSnVeU3Nxc9uxN49hjahMZWXh7sbGxNGvWjOhob5f/qpsDRzK447U5LP6+6LqWd1z8R4Zf0J168bFFHlNVicgKVU0MRlvxJ5ykp/5lpKdj1zw6utj3FZEewKOqeqH7fAyAqj5RxPH/BR5R1YXFtNkQ+CPOJblvVHW3p85SfPbtNRR+uVGc/mpnr29S2bZu3UrdunVp3rw5hWUON1VHbm4uqkpkZGSx+0rTXjCCUGB7QKFtqiqpqals3bqVFi3Ktbq1wh0+kkFkZAS1YoIbMOvF1eIff76EWyfP4r8bfinw+vXnJnBL3z9UyyBUIbyPG44TkeSA55NVdXLA8xOBXwOebwXOKKwhETkFaAEsLurNRGQOMA2YXZqadT7F3fjgeelduElPT7cgVE0U9gu+PIEkmEEIQESK/DkTERo2bMiuXbuC+p7Blp6exYwPv2HAhV1p1DD4I7cGdeJ46ZZLCwSj689N4L5Lz+GY2uW/ubgm8C3f9mh3sEZiwGDgPVXNKeaYp3Fuap0gIsuBGcDHqurpslS1XTVnQchUhpJ+zqrCz+GBQ0d48bUFTP73ItLKcF+VF75gdOZpJwMWhMLANuCkgOfN3H2FGQxML64xVf1cVW/DyaTwCnAN8LvXzhR3ae4gxV+aszQ/xlRhubm57DtwhKde+JiMzGxmfvQtl/f7Ayed2CAoKZDy8wWj2cvXcVn39haEyiCIU/rLgdYi0gInAA0Grst/kIi0BY4Fvi6pQXfV3ACckVFXwHMJoeJGRHVVtV4hW10LQiWLjIwkISGBjh07cvXVV3P4sLPMuE6dknOSnXnmmSUeE+ibb77hjDPOICEhgXbt2vHoo4/6X/voo4/o3Lkz7dq1o1OnTnz00Uf+15KSkoiPj+fgwYP+fSNHjkRE2L3b8zyjqYL27ktj6VfruebGfzJv0WoAcnJyueamF/j7s7PYuWs/RyrgfqgGdeIYfFZnC0JlFKz7iFQ1GxgBzMcpgvqOqq4VkXEiMjDg0MHADC1hVZuIvINTKvw84EWc+4ru8Pq5PCfHEpHGgH9GUVULzjwav7i4OFJSUgAYMmQIL7/8Mnfffbenc//73/+W6r2GDRvGO++8Q5cuXcjJyWHDhg0ArF69mtGjR7Nw4UJatGjBTz/9RN++fWnZsiWdOztrTU499VRmzZrF0KFDyc3NZfHixZx44omlen9TtWRl5aCq/Ofr9ezctT/Pa6rKqjVb2Lzld9q3PZG42Jigv39skBdE1ChBXOSsqnOBufn2PZzv+aMem5sCXFvCPFKRSpy5FZGBIrIR+An4HNgCzCvLm4WrefPm0b9/f/7whz/Qv39/5s0L7sc755xz2LRpU559hw4dok+fPnTt2pVOnToxa9Ys/2u+UdPSpUvp1asXV111FW3btmXIkCGFlgr+/fffadq0KeCMxNq3d24HePrpp3nwwQf9K7ZatGjBmDFjmDhxov/cwYMHM3PmTP/7nXXWWURFWfLO6iw6OpIGx9bhntv7MXfGvbRpdbz/tQfuGsDMV+/gj4mnUt9uzA0vYVqPyB2knAXMFJH3ROQxEWlSmja8LCH6G87a8P+5GVb7AN+Uurdhat68eYwfP54dO3agquzYsYPx48cHLRhlZ2czb948OnXKm2ImNjaWDz/8kJUrV7JkyRLuueeeQoPMqlWreO6551i3bh2bN2/mq6++KnDMqFGjOO2007j88st55ZVX/PdPrV27lm7duuU5NjExkbVr1/qft2nThl27drF3716mT5/O4MGDg/GxTRVQp3YsJ53QkKcfc6YGeiS2ZuCFXTn2mNpVYpFFTRRuKX5E5Cyc+SYF/u1uAN+6r3niJRBlqWoqECEiEaq6BAjWssCQmzRpUoEbX9PT05k0aVK52j1y5AgJCQkkJiZy8sknc+ONN+Z5XVV58MEH6dy5M+effz7btm1j586dBdrp3r07zZo1IyIigoSEBLZs2VLgmIcffpjk5GQuuOACpk2bxkUXXVSqvl5xxRXMmDGDb7/9lnPOOadU55qqr/Fx9eh3fhfG3nNZqVIemRAIt0gEzwCXqeojqjrb3R4BLgOe9dqIl2sw+0SkDvAF8LaI/A6U+oalcFXYL//i9nsVOEdUmLfffptdu3axYsUKoqOjad68eaGZIGrVquV/HBkZSXZ24ZmdW7Vqxa233srNN99Mo0aNSE1NpX379qxYsYIuXbr4j1uxYgUdOnTIc+6gQYPo1q0bw4YNC/p9Nib81a0TxyP3XhG+xW4MUOr7iCpLPVVdlX+nqqaISF2vjXj5rXMpcBgYhZNL6EecJXrVQpMmhV/KLGp/sOzfv5/GjRsTHR3NkiVL+Pnnst+a9cknn/gv623cuJHIyEiOOeYYRo8ezRNPPOEfRW3ZsoXHH3+ce+65J8/5p5xyCuPHj+e2224rcx9M1RYfF0OdOpbdINyF4RyRiMixhexsQClymRY7InIT432sqr2BXEqxLryquP322xk/fnye0UhsbCy33357MWeV35AhQxgwYACdOnUiMTGRtm3LXmvwrbfeYtSoUcTHxxMVFcXbb7/tXz7+5JNPMmDAALKysoiOjuapp54iISGhQBt/+ctfyvFpTFVnI+GqIfwGRPwDWCAio4GV7r5uwJPua56UmPRURBYBV6jq/mIPDLHApKc//PAD7dq183zuvHnzmDRpEjt37qRJkybcfvvtXHzxxRXVVVPDlPbn0VQfwUx6Gtv0JD152EhPx258svikp8HkVmK9D+iAEyvXARNVdY7XNrzMER0C1ojIQgLmhlT1ztJ1N3xdfPHFFniMMWFPw3BMpKofAx+Xpw0vgegDiq9DYYwxpqKFsPxqRSsxEKnqVDeH0MmquqES+mSMMaYQYbhqLii8ZFYYAKTgrJhDRBJEZLaXxksqRRtw3JUioiKSGLBvjHveBhG50Mv7GWOMqXq8LJV5FOgO7ANnfThOqu9ieSlF6x5XF7gL+DZgX3ucZHsdgIuAf7ntGWNMjeS7jyjMlm8XSUS6ej3Wa2aF/Cvmcj2c1x3YpKqbVTUTp1DSpYUc9zecpX6Bd3NeipPxNUNVfwI2ue0ZY6qwtCMZFVbzyISdW70e6CUQrRWR64BIEWktIi8AXtJDF1aKNk9aZzdinqSqn5T23HBXU8pAJCUl0aJFCxISEujatStff11i2ZI8vHw/KkPz5s3p1KmTPy2TqRgHDqZz4JCnop2mEFVpRKSqN3s91ksgugPnElkGTk3y/cDIMvUsgIhE4OQiuqekY4tp4xYRSRaR5HArx+xL8fP9998TExPDyy+/7PncspSBmDx5sv/9rrnmGuBoGYhZs2bxww8/MHv2bEaPHs13333nP9dXBgIotgzE0qVLSUpKKvT9J06cSEpKChMmTCj0xticnDJlhq90S5YsISUlBd/9aCa4Dhw6wlOvzefJVz/lwKEjns/LyskhNzdMfruGWvjlmgOc5KciUtt9PFREnhWRU7ye7yUQtVXVv6rqH9ztIY91yEsqRVsX6AgsFZEtOBm+Z7sLFjyVsVXVyaqaqKqJjRo18tClgrKysrjjjju44447OHz4sP9xVlZWmdorTE0pA9GzZ0//52zevDn3338/Xbt25d1332X69Ol06tSJjh07cv/99+c5b9SoUXTo0IE+ffpQ0h8UaWlp3HDDDXTv3p3TTz/d/3276667GDduHADz58+nZ8+e5ObmkpSUxPDhw0lMTKRNmzZ8/HG5bncwZZSZlc3O3QeY98X3fPqftfy2az+ZWYXnTQyUlZ3D5l172L5/vwWjMC0D4XoJOCwiXXAGFz9yNBN3ibwEomdE5AcR+ZuIdCxFx/ylaEUkBmfxgX+1naruV9XjVLW5qjbHKS0xUFWT3eMGi0gtt5Rta2BZKd7bs7vvvpuVK1eycuVK+vXr53/stYhdSWpSGYg5c+bk+ZwNGzZk5cqV9OzZk/vvv5/FixeTkpLC8uXL/ZcI09LS/H0699xzeeyxx4p9j/Hjx3PeeeexbNkylixZwr333ktaWhpPPPEEM2fOZMmSJdx555288cYb/rQ1W7ZsYdmyZXzyyScMHz6c9PR0tm/fTr9+/fztiggXXHAB3bp1Y/LkyWX+Hpi8cnJy2bM/jZlzk7n27tf8+6+7ZwozPlnOnv1p5OQUPuWclZ3Dj7tSGfradIa8Op3t+w9YMApf2W4V10uBF1V1Es5gw5MSA5GbZ643sAt4RUTWiMhDHs7zWoq2sHPXAu/gpIr4FLi9rJX/vMrIyODQoUNkZARnIrW6lIHwzT3ddNNNzJ49m4SEBBISEpg/f77/mHvvvZeEhAQmT57MlClT/PsHDRoEwPLly+nVqxeNGjUiKiqKIUOG8MUXXwBOjjPfcUOHDuXLL78stq8LFixgwoQJJCQk0KtXL9LT0/nll1+Ij4/n1VdfpW/fvowYMYJWrVr5z7nmmmuIiIigdevWtGzZkvXr13PCCScwd+7R4pRffvklK1eu9Kd78vXPlN8j/5zN+JfmcihgkULa4Qwef3keY5+bVeiVJF8Qun7KDA5nZrH70GGGvDqtxgejMB4RHRSRMcBQ4BN36sVzKV5P12BUdQfwTxFZgpNT6GHg7x7OK7EUbcD+XvmejwfGe+lfeTz55JP069cvTwDyJQctj+pSBuLbb51V9UuXLuXNN9/kzTffLPDeEydO5Kqrriqwv3bt2kV+/qKUVJBNVXn//fc57bTTCry2Zs0aGjZsyPbt24tts7D38M2LNW7cmMsvv5xly5bRs2fP0nbf5BMZGcG4uy7l2v7dGfvcLLbt3AfAiU2OYdydA2nf+gSiIvP+zOUPQj6+YPT2zddxQv16RETUtOJ9YbQSoaBBwHXAjaq6Q0ROBiaWcI6flxta24nIoyLyPeBbMdesrL0NN/fff3+B+aCsrCzuu+++Cn3fmlQGonv37nz++efs3r2bnJwcpk+fzrnnngs4CyTee+89AKZNm8bZZ58NwIsvvsiLL75YoK0LL7yQF154wf95V61ySqH8/PPPPPPMM6xatYp58+b5AyjAu+++S25uLj/++CObN28uEMTS0tL8KwfT0tJYsGABHTuW5iq0Kc6x9eLpkdCSfz/1ZyIjIoiMiGDqk3/mzK6tOLaQQnxb9+0vEIR8fMEoNa3alEQrlTBdqwBOEJqpqv8BUNVfVNXzHJGXEdHrOPcAXaCq20s6uKqqVasW0dHRQV2kUJyaVAaiadOmTJgwgd69e6OqXHLJJVx6qXNLWe3atVm2bBl///vfady4sX/hxPr16znrrIKVhseOHcvIkSPp3Lkzubm5tGjRgjlz5nDjjTfy9NNPc8IJJzBlyhSSkpJYvnw5ACeffDLdu3fnwIEDvPzyy8TGxrJ9+3Zuuukm5s6dy86dO7n88ssBZ07vuuuuK/XlTVM8EeGYevFc1787uZrLsfXiixz9HhMfR8cTj2fZT78W+nqPlqcQE1lD728P2wERdXHKQewBZgLvqqrn6qJeykDEAb4L7ps8rpirdGUtA5GVleVfmPDkk0/6V3Q9++yzREd7vsRpgqx///588MEHxMTElKudpKQk+vfvX+ilw8piZSCO2rXHGXk2alD8PPbew0e4e+acAsFoQOd2jOnXm/rxcRXWx2AKZhmIWk2a6fGD7vJ07C8v3FdpZSACiUhnnMt0VwJbVfV8L+cVOSISkSjgceDPwC+AACeJyBvAX1W1coYOFSw6OpoXXnjB/zzwsQkdW2ZdPdWtE1viX/W5ucqx8XE8O2hAnmBU1YJQDfU7sANIBRp7Pam4S3MTcYZbLVX1IICI1AOedjdvodmYECpscYUJndiY4q8yHMnIBCCuVkyeYNSkbp0aH4TCKWtCfiJyG3AN0Ah4F7hZVdd5Pb+4QNQfaKMB1+5U9YCI3AqsxwKRMSbI9h9K594X5/DC3VdwTN04jo2P4x+DBiBQo4NQFXASMNJNil1qxa2aUy1kAsm9nydM47Ixpqo6eDidif+3hP9+9xMbtx7NsHFMfJwFIZ8wXTanqmPKGoSg+EC0TkT+lH+niAzFGREZY0xQ7DlwmMXJG5n1nzUA3PvCbDb9upsDaWG5Nip0wjQQlVdxl+ZuBz4QkRuAFe6+RCAOuLyiO2aMqf72H0pn8/bd3P/iHH7clurf/+vOfVw48mUuObM9D91wAXXjaxFXy1axlrTKuaoqckSkqttU9QxgHLDF3capandVLZCA1ORlZSC8CZcyEDfccAONGzcucCPrnj176Nu3L61bt6Zv377s3bs3RD2snmrFRNKs0TGc2bkFkfkyJTSsX5s+f2hDVGSEBSGfMB0RiUhtN60PItJGRAaKiOd/NC+55har6gvutqg8na1JrAzEUVWhDERSUhKffvppgf0TJkygT58+bNy4kT59+jBhwoQQ9K76io2JptGxdRg9pDfzn7/VH3DO7NycT/9xC/3Pak+DQrIvmLDzBRArIicCC4DrgTe9nuwl+7YpJysDEf5lIHr27EmDBg0K7J81axbDhg0DnIAfOKI0wVMnrhYnHFeP2686m4gIYfzwS2hQvzaRkfYrys/raCg0V+9EVQ8DVwD/UtWrcerYeWL/yji/hBITE/1bMJNdWhmIqlEGoig7d+70B/njjz++0AzpJjhqxURxTZ8Ehl9+po2CihLGgUhEegBDAF/Fbc95mCwQgX/+pqjnZWFlIKpWGQgvRKTE7OCmfOrXiWXEVedQJ65WsccdTE9nT1oaObmF1zIyle4uYAzwoVvupyWwxOvJZbsGY0pkZSCqVhmIojRp0oTffvuNpk2b8ttvv9G4seesJaYMoiIjiSrh7+iD6em8vTyZ2Wu+4+XBgznxmGOIjKghf1OH4aI5EYnEKWrqrzOnqpuBO722UUP+9cKPlYEInzIQxRk4cCBTp04FYOrUqf6s4SY0fEHo7eXLOZiewfAZM9i2b1/NGRmFYWU8N8nB2eVpwwIREB8fX+zzijBkyBCSk5Pp1KkT//73v8tdBuK0004jISGB66+/vtAyEG3btmXAgAHFloEIvJwVTIFlILp06UK3bt0KlIHo2LEjixcv5uGHnbqJ69evp2HDhgXaGjt2LFlZWXTu3JkOHTowduxYVLVAGYibbrrJP8L0lYG4+OKL85SBCJwjuvbaa+nRowcbNmygWbNm/kuMDzzwAAsXLqR169Z89tlnPPDAAxXyPTIlCwxCR/fVwGAUnlaJyGwRuV5ErvBtXk8usQxEVVHWMhAmPFkZCBOosCAUqG5srbC8TBfMMhAxjZppk0vv8HTs1ikPVGoZCLcqQ36qqjd4Od/miExYsjIQJpACP6WmFvn6kcwsUtPSaFy3blgFomAL16Uyqvrn8pxvgchUa1YGonqoFxvLgxdewOPzF/DfzZv9++OioxnYqRNntmpJu+OPJ7Y6F7NUIEyvPopIM+AFwFdW+T/AXaq61cv51fdPB2NMtVI/Lo4HL7yAM1u29O8b3C2RIX/oToemTat3EPIL3o1EInKRiGwQkU0iUujkp4hcIyLrRGStiEwrprk3gNnACe42x93niQUiY0yVERiMGtWpQ582p3HNCzNIS88MddcqR5DikLvkehJwMdAeuFZE2uc7pjXOvUFnqWoHYGQxTTZS1TdUNdvd3sQpkueJBSJjTFg7nC/I+ILRpGsGMWH2F/zvt938Z8PPNWPVXPAGRN2BTaq6WVUzgRlA/nsTbgYmqepeAFX9vZj2UkVkqIhEuttQnHLhnlggMqYGy8rOKfRxRcnJySE9M8vz8Xv2p3H4SN5AlJOTS3a28lHyD8xd/T8AHn7vM75Yv4U9h44Etb9V2HEikhyw3ZLv9ROBXwOeb3X3BWoDtBGRr0TkGxEpLmXLDTilwne421WA5wUMtlghyFJTU+nTpw8AO3bsIDIykkaNnBHqsmXLyrQcuU6dOhw6dCio/TQmIzOb7JwcoqMiycnJJSMzGxEhqgITje4/lI6qEhvjbT5n9cbtLPj6B/5644XUia/FwfQMVv/8GyPfmsuug2n+4/YdTifplfcZ078nV/yhg1O/yON7VBUCiPe7bXYHYfl2FNAa6AU0A74QkU6qui//gar6MzAw/36vbEQUZA0bNiQlJYWUlBSGDx/OqFGj/M/Le0+MMUU5cOgIO1MPFLodKGKUcCgtnRseeJO9+9PYf+gIfxo9hQMHK25EkZOTw5Jl/2P6J8vJ8DAq2rM/jSdeX8CHi1dz8LBzc3LtmBjan9iYuy7qQb18+eg6n9SEAae3RaDaBSHAvewWtMwK24CTAp43c/cF2grMVtUsVf0J+B9OYCpARFqKyBwR2SUiv4vILDffnCcWiFy+DNzBzLzts2jRIk4//XQ6derEDTfcQEZGBuCUSrjvvvvo1KkT3bt3L1AqIr/du3fTo0cPPvnkk2KPMzXPkYwszr1uYqHbkYyCv/QzMrOZ8clyVq/fyo5d+/nP8v+xbtNvfPbVOrJzKmauZf+hdP7x5mdMef+/HEzLKPH41Ru389O2VHJylX9O/5xDhzOIiBAa1Iln8B87sejBG2hU18lneH7HVkz9y1W8Oucb3l26mkPpJbdfwy0HWotICxGJAQbjrHoL9BHOaAgROQ7nUt1mCjcNeAdoirNq7l1gutfOWCBy+TJuByPzdqD09HSSkpKYOXMma9asITs7m5deesn/ev369VmzZg0jRoxg5MiRRbazc+dOLrnkEsaNG8cll1wS1D6amudQWjqvv/clIoICz0xZCMDzUxdVyKjINxratfcQh9MzmTG3+FGRbzTkEzgqAoiOiqJh7XjGXt6LyAhh3FV9yMrKYdpnK3l51tccTMuolDmvShekxQqqmg2MAOYDPwDvuFmzx4mI7xLbfJxFCOtwMmnfq6pFLUCIV9W3AlbN/R8Q6/Vj1fhA5BsJRUY6KX8jIyODOjLKycmhRYsWtGnTBnCKq/lKIICT48z3tagy21lZWfTp04ennnqKvn37BqVfpubyjYbSjmRyfo92rPz+F35PdcrFp+5Lq5BRkW805FPSqMg3GvIJHBX5REZGcM5pzXn48vOoHR3DS7P+S1ZOLgePZDBt0SoOpWeSm1s9Upj5BbEekarOVdU2qtpKVce7+x5W1dnuY1XVu1W1vap2UtUZ+dsQkQYi0gCYJyIPiEhzETlFRO4DPNdZqfGByDcC8pWz9n0N9sioKIElCUSEnJwcf80fXwLQqKgounXrlqcGkDFlFTgaumnQ2Uye+UWe14M9KsrJyWHJtxvYtffogpviRkX5R0M++UdFAA3qxHNtj85kZGYz7bOV/v1T5y0n7UgGhzIKllapysTjVolWAMk4K+b+gjNyWgrcCgzy2kiND0S+TNuBI6LA/eUVGRnJli1b/PM/b731lr8EAuAv0z1z5kx69OhBZGSkf3GDr/S1iPD666+zfv16nnzyyaD0y9RM+UdDa9Zv84+GfII9Ktp/KJ1/TF1UYH9Ro6L8oyGfwkZF4Czn/tesr8gK6O/BIxnMXLKaIxnZFTbnVfkUcj1uldUj1Raq2tL9mn+zxQpeffHFFyQnJ+cZESUnJ+e5fFYesbGxvPHGG1x99dV06tSJiIgIhg8f7n997969dO7cmeeff55//OMfRbYTGRnJ9OnTWbx4Mf/617+C0jdT85Q0GvIJ1qiosNGQT2GjoqJGQz75R0VpGZnsS0tn+merChw7dd5yMrJyOGgLFyqFiJwpIteJyJ98m9dzK/Q+IvcGqOdxape/pqoT8r0+HLgdyAEOAbeo6joRaY4zgbbBPfQbVR1OBYqPj+fw4cNBrUX06KOP+h/7Crjld++995Y4yvHdQ1SrVi27PGcKFVcrms+n3Vvka5B3NNT3zPaFjoZ8fKOiKy7qVq77iooaDflMef+/DO73B2q5y63r1YljyiPXFdtm3fhYcnJziYyIIDMrm5fduaH8Dh7J4N0lqxl6QVcys3OIKan0a7jTUt1HVKlE5C2gFZCC8/scnNmqf3s5v8ICUUAuo74469GXi8hsVV0XcNg0VX3ZPX4g8Czgu3v3R1VNqKj+5ResEZAxoVCvThz16sQVe0xubi49/9Ca7p1bcMqJDdm15yBvPJmU55gIESIiBBGhdnwtMjKyiIqvVXiDHtSJq8X/PVV8SZr4uKP310VFRnBSk2M9t5+ZmcOMRYX/kQfOqOjq3l04nJFJTFTx358qIXzrxyUC7bWMBe4qckTkz2UEICK+XEb+QKSqBwKOr01YVmSvOL4S3sZUhrjYGDqd1sz/vHHDuhX+njExUTQ/sWCl3WCJqxXNly+OIDM7l72HCl9gJEBGljNXVJFZI2q474Hjgd/KcnJFBqLCchmdkf8gEbkduBuIAc4LeKmFiKwCDgAPqep/Cjn3FuAWcMpBG2Nqlnq1Y8nWXEa+MZc1P+8o8rjRl5/DpX9oxzG1q/ioKMz+VBeROTi9qgusE5FlgH9STlU9pf0Jea45VZ0ETBKR64CHgGE4UfVkVU0VkW7ARyLSId8IClWdDEwGp1R4JXfdGBMGoiMjef6m/gCkZ2bxc+q+Qo9b/9suEls0q9KjojCcI3o6GI1UZCDyksso0AzgJQBVzcCNqqq6QkR+xEkvkVwxXTXGVFV142pRN64Wuw+mce8780jeUvivmQa14/j0nj/ToE7pFiRlZmeQneus7IuKiCYmquxzZuUXXpFIVT8PRjsVGYj8uYxwAtBgIM9yGBFpraob3aeXABvd/Y2APaqa4ybOa03ROY6MMTXYwYx09qal0bB2Xc7v0Ire7fLevhIZEUFsdBQCHMnKJjdXiYjwfttndm4WH30/BYDLOt5IDCEMROEVh/xE5CAFe7cfZ/Bwj2+tQFEqbIzqMZfRCLcEbQrOPNEwd39P4Dt3/3vAcFXdU1F9rQiRkZEkJCTQsWNHBgwYwL59+4o9PiUlhblzj2bEmD17NhMmTCjmDO+++eYbzjjjDBISEmjXrl2eZeUfffQRnTt3pl27dnTq1ImPPvrI/1pSUhLx8fEcPHh0ie/IkSMREXbv3h2UvhlTHrnuIq3/btnML3v3sGbnr6zbtTXPtmbnL5zfoRVDzjydE4+tV6ogFFbc5dtethB4DrgXZ21AM2A0TiLUGcDrJZ1coXNEqjqXfPmGVPXhgMd3FXHe+8D7Fdm3ihYXF0dKSgrg5JebNGkSf/3rX4s8PiUlheTkZPr16wfAwIEDGTiwzOU98hg2bBjvvPMOXbp0IScnhw0bnNuzVq9ezejRo1m4cCEtWrTgp59+om/fvrRs2ZLOnTsDcOqppzJr1iyGDh1Kbm4uixcv5sQT89fPMiY0VJXF/1tH4kmn8OT8xXy/vfBFW08uWMTYfhfQoHbtSu5hkIXpiAgYqKpdAp5PFpEUVb1fRB4s6eSqO2sXZMnJyfTt25fk5OBPQ/Xo0YNt25zr1suWLaNHjx6cfvrpnHnmmWzYsIHMzEwefvhhZs6cSUJCAjNnzuTNN99kxIgRgDMyufPOOznzzDNp2bIl7733HuDcF3LbbbfRtm1b+vbtS79+/fyvBfr9999p2rQp4IzU2rd3StM//fTTPPjgg7Ro0QKAFi1aMGbMGCZOnOg/d/Dgwf40REuXLuWss84iKirka1yMAZzLbuef1oE6tWKpG1uLHi2bF7pFSOmysGVmZ3A48xCHMw+RkX00w0RG9hH//sOZh8jMruysDUHMehpch0XkGhGJcLdrAF8KjBI7ZL9RcILQyJEjSU9PZ+TIkTz33HMkJpa3uKEjJyeHRYsWceONNwLQtm1b/vOf/xAVFcVnn33Ggw8+yPvvv8+4ceNITk7mxRdfBODNN9/M085vv/3Gl19+yfr16xk4cCBXXXUVH3zwAVu2bGHdunX8/vvvtGvXjhtuKHjz4KhRozjttNPo1asXF110EcOGDSM2Npa1a9cyevToPMcmJiYyadIk//M2bdowe/Zs9u7dy/Tp0xk6dCjz5s0LyvfGmGCoHVMLVHjo4gs4mJ7Otv37aFi7Dlv27CY17ehl5RW/bqFHi1bUqVXyHE/gvFCgeeun5XlemXNGAkj4ps0bgpNF5184gecbYKiIxOFM0RSrxgeiwCAEBC0YHTlyhISEBLZt20a7du385Rv279/PsGHD2LhxIyJCVlbJlSoBLrvsMiIiImjfvj07d+4E4Msvv+Tqq68mIiKC448/nt69exd67sMPP8yQIUNYsGAB06ZNY/r06SxdutTzZ7niiiuYMWMG3377La+88orn84ypLLVrxRAbHUVWTjYnNziWbfv38fwXCwsc16bx9Z4CUfgKz2tz7mKEAUW8/GVJ59f4QDRmzBh/EPJJT09nzJgxLFxY8AfZK98c0eHDh7nwwguZNGkSd955J2PHjqV37958+OGHbNmyhV69enlqr1bAf56yZNFo1aoVt956KzfffDONGjUiNTWV9u3bs2LFCrp0OXppd8WKFXTo0CHPuYMGDaJbt24MGzaMiAi7mmvCU2ZONks2bWDbvj38fuggTerWK3DMrDWr+PMZZ1M3tviabVER0VzW0bmKkZF9xD8SurjtddQKSBUUFVGJJclDdtWtaCJyn6o+JSIvUEjvVPVOL+3U+ED0xBNP5BkRgZMx+4knnghK+/Hx8fzzn//ksssu47bbbmP//v3+yf7Ay29169bNszrNi7POOoupU6cybNgwdu3axdKlS7nuuoIJIz/55BP69euHiLBx40YiIyM55phjGD16NFdffTXnnXcezZs3Z8uWLTz++OMF5plOOeUUxo8fz/nnn1/6b4AxlUQVLmzbgVxVd06ocF5WzcVE1Sr0klutqDjiY+qUq5/lEYY3tP7gfi3X5HqND0SJiYk899xz/mAUGxsb1DkigNNPP53OnTszffp07rvvPoYNG8bf//73PCW/e/fuzYQJE0hISGDMmDGe2r3yyitZtGgR7du356STTqJr167Ur1+/wHFvvfUWo0aNIj4+nqioKN5++23/8vInn3ySAQMGkJWVRXR0NE899RQJCQkF2vjLX/5S5s9vTGWIj4khPiam5AOrrPAbEqnqHPfrVAARiVfVUlcVlTImSw07iYmJ6lvx9sMPP9CuXbtSnZ+cnMyYMWN44oknghqEKtqhQ4eoU6cOqampdO/ena+++orjjz8+1N0yAcry82jCR3kyK4jIClUNyi+U2GNO0JN6evuDcNOcR4P2vl6ISA9gClBHVU8WkS7AX1T1Ni/n1/gRkU9iYmK55oRCpX///uzbt4/MzEzGjh1rQciYICvqMl0ohOGlOZ/ngAuB2QCqulpEeno92QJRFVea1W/GmKoufCORqv4qeefmcoo6Nj8LRMYYU1WEbxz6VUTOBFREooG7OLqQoUS2FtcYY6oCr0kVQhOshgO34+Sa2wYkuM89sRGRMcZUERKmi8tUdTdOdoUysUBkjDFVRZjFIRF5uJiXVVX/5qUduzRXQawMxNE2WrRoQUJCAl27duXrr78uVd/r1AndzYOBmjdvTqdOnUhISKhSy/tN9SKqnrZKlFbIBnAjcL/XRiwQVRBfip/vv/+eBg0a5EkkWpj8gWjgwIE88MADQenLsGHDmDx5sr8/11xzDXC0DMSsWbP44YcfmD17NqNHj+a7777zn+srAwEUWwZi6dKlJCUlFfr+EydOJCUlhQkTJhR6Y2xOjufFNSG1ZMkSf7kOYwyo6jO+DZgMxAF/xqlD1LLYkwNYIHLt3LmTgQMH+hOKBpOVgXD07NmTTZs2Ac4I4/7776dr1668++67TJ8+nU6dOtGxY0fuvz/vH1KjRo2iQ4cO9OnTh127dhX7Hmlpadxwww10796d008/3R9E77rrLsaNGwfA/Pnz6dmzJ7m5uSQlJTF8+HASExNp06YNH3/8cZk+mzGVQtXbVolEpIGI/B34Dme6p6uq3q+qv3ttwwKR67nnnmPHjh08//zzQW3XVwbCV+TOVwZi1apVjBs3jgcffJCYmBjGjRvHoEGDSElJYdCgQQXa8ZWB+Pjjj/0jpcAyEG+99VaRl718ZSAuv/xyXnnlFX9evbVr19KtW7c8xyYmJrJ27Vr/8zZt2rBr1y5/GYjBgweX+XsxZ84cOnXq5H/esGFDVq5cSc+ePbn//vtZvHgxKSkpLF++3H+JMC0tzd+nc889l8cee6zY9xg/fjznnXcey5YtY8mSJdx7772kpaXxxBNPMHPmTJYsWcKdd97JG2+84U/gumXLFpYtW8Ynn3zC8OHDSU9PZ/v27f4ihQAiwgUXXEC3bt2YPHlymb8HxpRZGK6aE5GJwHLgINBJVR9V1b2lbafGB6KdO3cyZswYPv/8c3Jzc1m6dCljxowp98jIVwbi+OOPZ+fOnXnKQFx99dV07NiRUaNG5fmlX5zyloFITk7mggsuYNq0aVx00UWl+iyBZSDOOeecPK/55p5uuukmZs+eTUJCAgkJCcyfP99/zL333ktCQgKTJ09mypSjNV58AXf58uX06tWLRo0aERUVxZAhQ/jiiy8AiIiI8B83dOhQvvyy+IzyCxYs8Ofs69WrF+np6fzyyy/Ex8fz6quv0rdvX0aMGEGrVq3851xzzTVERETQunVrWrZsyfr16znhhBPyXCr98ssvWblyJfPmzWPSpEn+/hlTWaQUWyW6BzgBeAjYLiIH3O2giBzw2kiNXzV38803s2PHDnJznYpTmZmZLFq0iLVr1zJ79uwyt1tTykB8++23gHPZ7s033yxQ0A+cOaKrrrqqwP7aZSjbLMVkVQbne/P+++9z2mmnFXhtzZo1NGzYkO3btxfbZmHv4ZsXa9y4MZdffjnLli2jZ0/PGUyMCYLKv+xWElUNymCmxo+IXn31Vc4//3xi3Ky9MTExnH/++bz66qtBad9XBuKZZ54hOzs76GUg3n//fXJzc9m5c2eR6X4++eQTf/DKXwbiiSeeYMuWLQD+MhD33HNPnvN9ZSBuu81T/sJS6969O59//jm7d+8mJyeH6dOnc+655wLOPJhv3mvatGmcffbZALz44ov+araBLrzwQl544QX/5121ahUAP//8M8888wyrVq1i3rx5/gAK8O6775Kbm8uPP/7I5s2bCwSxtLQ0/79NWloaCxYsoGPHjkH+LhjjQZhdmguWGh+ImjRpwuOPP865555LREQEvXr14vHHH6dJkyZBe4/8ZSDGjBnD6aefTnZ2tv+Y3r17s27dOv9iBS+uvPJKmjVrRvv27Rk6dGixZSBOO+00EhISuP766wstA9G2bVsGDBhQbBmIwMtZwdS0aVMmTJhA79696dKlC926dePSSy8FnFHTsmXL6NixI4sXL+bhh53bFtavX0/Dhg0LtDV27FiysrLo3LkzHTp0YOzYsagqN954I08//TQnnHACU6ZM4aabbvLPlZ188sl0796diy++mJdffpnY2Ng8c0Q7d+7k7LPPpkuXLnTv3p1LLrmk1Jc3jQmKMFysEAxWBsK1c+dObr75Zl599dWgBqGKVlPLQPTv358PPvjAP5Itq6SkJPr371/opcNgsTIQNVcwy0DE1WuqLf5wg6djf1j8eKWWgSivGj9H5NOkSZNyzQmFSk0tA2HLrE2NVD3GDQVYIKrirAxE+RS2uMKYsKRUyctuXlggMsaYKiJck56WlwUiY4ypKqpnHLJAZIwxVYYFImOMMaFVPSNRjb+PqKJYGYijbVSHMhA33HADjRs3LnAj6549e+jbty+tW7emb9++7N1b6jRbxngXxBtaReQiEdkgIptEpECqfxFJEpFdIpLibjcF6VMUYIGoglgZiKOqQxmIpKQkPv300wL7J0yYQJ8+fdi4cSN9+vQJ2h8PxhTg9WZWDwsaRCQSmARcDLQHrhWR9oUcOlNVE9ztteB+oKMsELl2797NpZde6vkv/dKwMhCOqlwGomfPnjRo0KDA/lmzZjFs2DDACfiBI0pjgi2ISU+7A5tUdbOqZuLUD7o0+D32pkIDkYeh33ARWeMO+74MjMgiMsY9b4OIXFiR/QR47bXX2L59O6+9Ftygb2UgjqrKZSCKsnPnTn+Q92VaN6bCeL80d5yIJAdst+Rr6UTg14DnW919+V0pIt+JyHsiclJQP0uACgtEHod+01S1k6omAE8Bz7rntgcGAx2Ai4B/ue1ViN27dzNnzhxUlTlz5gRlVGRlIKpXGQgvRKTE7ODGlIv3S3O7VTUxYCtLEa05QHNV7QwsBKYG86MEqsgRUYlDP1UNrFdRm6PTbJcCM1Q1Q1V/Aja57VWI1157zV8GIjc3NyijIt8c0c8//4yq+ueIfGUgvv/+e+bMmeMfnZQkWGUgFi1axOrVq/OUgQhUVBmIsWPH0rdv30LLQKSkpPDaa68xcOBAUlJSSElJ4cILjw5ifXNECxcuzDPZX5FlIHz9+OWXX/x53spTBqIoTZo04bfffgOcUWvjxo09n2tMqQVvscI2IHCE08zdd/StVFNVNcN9+hqQ9/JJEFVkIPI09BOR20XkR5wR0Z2lOTcYfKOhrKwsALKysoI2KgIrA+FFuJeBKM7AgQOZOtX5Q3Hq1Kn+rOHGhLnlQGsRaSEiMThXoPIk2xSRpgFPBwI/VFRnQr5YQVUnqWor4H6cKn+eicgtvmugJU1iFyVwNOQTrFGRj5WBKF64l4EAuPbaa+nRowcbNmygWbNm/kuMDzzwAAsXLqR169Z89tlnQVvpaEwBCuSqt62kplSzgRHAfJwA846qrhWRcSIy0D3sThFZKyKrcQYJSRXzwSqwDISI9AAeVdUL3edjAFT1iSKOjwD2qmr9/MeKyHy3rSJvQilrGYiLL7640JVYjRo1Yt68eZ7aCCUrA2FlIEz4CmoZiNrH66kdrvd07PfLn7YyEC7/0A/n2uNg4LrAA0SktapudJ9eAvgezwamicizOPXQWwPLKqKTVSHYFMfKQBhTk1TPzAoVFohUNVtEfEO/SOB139APSFbV2cAIETkfyAL2AsPcc9eKyDvAOiAbuF1Vw/+uxxCwMhDlY2UgTJVSPeNQxeaaU9W5wNx8+x4OeHxXMeeOB8aX471tKa0JuepSAdmEiWr64xTyxQoVITY2ltTUVPslYEJKVUlNTSU2NjbUXTHVRZBS/ISbapl9u1mzZmzdurXEdDDGVLTY2FiaNWsW6m6Y6qAUCU2rmmoZiKKjo/3504wxxoS3Clu+XdlEZBfwcylPOw4IfpbTilcV+219rjxVsd/Vtc+nqGqjYLyZiHzqvqcXu1W1dLm8QqjaBKKyEJHkqrTW3qcq9tv6XHmqYr+tzzVbtVysYIwxpuqwQGSMMSakanogKktq9HBQFfttfa48VbHf1ucarEbPERljjAm9mj4iMsYYE2IWiIwxxoRUjQhEInKRiGwQkU0iUqBgjIjcLSLr3Nrsi0TklFD0M1+fSurzcBFZIyIpIvJlIWXYQ6Kkfgccd6WIqIiEfPmrh+91kojscr/XKSJyUyj6ma9PJX6fReQa9+d6rYhMq+w+FsbD9/ofAd/n/4nIvhB0M3+fSurzySKyRERWub9D+hXWjimGqlbrDSfz949ASyAGWA20z3dMbyDefXwrMLMK9LlewOOBwKdV4XvtHlcX+AL4BkgM9z7jFAR7MdTf31L2uTWwCjjWfd64KvQ73/F34GTtD+s+4yxauNV93B7YEurvdVXbasKIqDuwSVU3q2omMAPIU89ZVZeo6mH36Tc49dtDyUufDwQ8rU14ZKEqsd+uvwFPAumV2bkieO1zOPHS55uBSaq6F0BVf6/kPhamtN/ra4HpldKzonnpswL13Mf1ge2V2L9qoSYEohOBXwOeb3X3FeVGINTV8jz1WURuF5EfgadwSvmGWon9FpGuwEmq+klldqwYXn8+rnQvu7wnIidVTteK5KXPbYA2IvKViHwjIuGQ7sXz/0X38ngLYHEl9Ks4Xvr8KDBURLbilL25o3K6Vn3UhEDkmYgMBRKBiaHuixeqOklVWwH3Aw+Fuj8lccvBPwvcE+q+lNIcoLmqdgYWAlND3B8vonAuz/XCGVm8KiLHhLJDpTQYeE+rRkHMa4E3VbUZ0A94y/1ZNx7VhG/WNiDwL9hm7r483EqxfwUGqmpGJfWtKJ76HGAGcFlFdsijkvpdF+gILBWRLcAfgdkhXrBQ4vdaVVMDfiZeA7pVUt+K4uXnYyswW1WzVPUn4H84gSmUSvNzPZjQX5YDb32+EXgHQFW/BmLxnpzUQI1YrBAFbMYZ5vsmGzvkO+Z0nAnJ1qHubyn63Drg8QCc8uth3+98xy8l9IsVvHyvmwY8vhz4pgr0+SJgqvv4OJzLSw3Dvd/ucW2BLbg33Id7n3Eu5Se5j9vhzBGFvO9VaauW9YgCqWq2iIwA5uOsgHldVdeKyDicX96zcS7F1QHedcuL/6KqA8O8zyPcUVwWsBcYFqr++njsd1jx2Oc7RWQgkA3swVlFFzIe+zwfuEBE1gE5wL2qmhq6Xpfq52MwMEPd3+yh5LHP9+Bc+hyFs3AhKRz6XpVYih9jjDEhVRPmiIwxxoQxC0TGGGNCygKRMcaYkLJAZIwxJqQsEBljjAkpC0QGABFpJiKzRGSjiPwoIs+LSIzHc5eW9qZUERnnLj+vNCKyRUSqxI2GbiqhliUc86iIbHMzVW8UkQ8Cs7CLSLSITHBfWykiX4vIxe5rdUTkJfffeqWIrBCRm93XGonIpxX7CY05ygKRQZybpz4APlLV1jh5yuoA4ws5ttz3nolIpKo+rKqflbetwtoOdpuVTUQ6AJGqutnD4f9Q1QT3320msFhEGrmv/Q1oCnRU1a442Tfquq+9hnP/WWv3tYuABgCqugv4TUTOCtZnMqY4FogMwHlAuqq+AaBOfq9RwA0iEu/W45ktIouBRSISJyIzROQHEfkQiPM1JCIXuH95rxSRd0Wkjrt/i4g8KSIrgatF5E0RuSrgtcfcc9aISFt3fyMRWejW03lNRH4ubEQjIodE5BkRWQ30EJGhIrLMHSm8UlhwKuwYcWo8TQw4JklEXnQff+SOGtaKyC353nu8iKwWJ7loE3d/ExH50N2/WkTOLOp9C/n3GALMKuk98lPVmcAC4DoRicfJwH2HuumJVHWnqr4jIq1wsko/pKq57mu7VPXJgOY+cvthTIWzQGQAOgArAneoU2biF+BUd1dX4CpVPRenZtNhVW0HPIKbe80NEg8B57t/ZScDdwc0m6qqXVV1RiF92O2e8xIw2t33CLBYVTsA7wEnF9H/2sC3qtoFSAUGAWepagJOVoE8v1BFpF0Rx7yPk8LHZxBOHj+AG1S1G05S3DtFpGHAe3/jvvcXOL/8Af4JfO7u7wqsLeZ98zuLvP8eRb1HYVbipMg5FSdDyIFCjukArPYFoSIkA+cU87oxQVPtU/yYoFmoqnvcxz1xftGiqt+JyHfu/j/iFAb7yrnaRwzwdUAbM4tp/wP36wrgCvfx2biBQVU/FZG9RZybgxNEAPrgBMblbh/igPy1eAo9RlV3ichmEfkjsBHnF/pX7jl3iogvSJ2Ek0A0FcgEPg7oe1/38XnAn9y+5wD7ReR6D30D53LaroDnRb1HYaSY1wo/QeSvwNU4xfNOcHf/DpxQ9FnGBI8FIgOwDrgqcIeI1MMZgWzC+Ys+zUM7ghOwri3i9eLa8GW3zqH0P5fperRcgOAk+xxTzPHFHTMDuAZYD3yoqioivYDzgR6qelhEluJkWAbICsgrVlLfvfQN4EhA+6V9j9NxRjObgJNFpF4ho6J1QBcRiVDVXFUdD4wXkUMBx8S6/TCmwtmlOQOwCIgXkT+Bf8L/GZwaK4cLOf4L4Dr32I5AZ3f/N8BZInKq+1ptEWlTjn59hRMUEJELgGM9fparRKSxe14DcYqseT3mQ5wKnNdy9LJcfWCvG4Ta4oz8vPTjVrf9SBGp77FvAD9w9JKoZyJyJXABMN39d5sCPC/u6kd3zu1qVd2EE6z+7pujEpFY8o6m2gDfl7YPxpSFBSKD+9f25TiLCDbi1K5JBx4s4pSXgDoi8gMwDnc+w11tlQRMdy/XfY1zeausHsPJIP09zqWjHcDBEj7LOpx5qgVuHxbiXOrydIw6pbV/AE5R1WXuKZ8CUe7nnYATcEtyF9BbRNbgfH/ae+mb6xOcgnZejHIXPmwEhgLnuf8OuO+1C1jnfg8/Bnyjo5uAhsAmEUl2+3JfQLu93X4YU+Es+7YJWyJSC8hxU/H3AF5yJ/mrNRGJA5bgLGoISYVSEfkCuNQNzMZUKJsjMuHsZOAdccouZ1L8arFqQ1WPiMgjwIk4KxcrlTj3IT1rQchUFhsRGWOMCSmbIzLGGBNSFoiMMcaElAUiY4wxIWWByBhjTEhZIDLGGBNS/w8QXDymfPA5oAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ax = sns.scatterplot(x=meanResReduced.nDCG.values, y=meanResReduced.meanOverallDiversity.values, \n",
    "                hue=meanResReduced.neighborOverallRatio.values, \n",
    "                style = zip(meanResReduced.index.get_level_values(1),meanResReduced.index.get_level_values(3),meanResReduced.index.get_level_values(2)),\n",
    "                palette='crest_r',s=100)\n",
    "\n",
    "norm = plt.Normalize(meanRes.neighborOverallRatio.min(), meanRes.neighborOverallRatio.max())\n",
    "#meanRes.neighborOverallRatio.min(), meanRes.neighborOverallRatio.max()\n",
    "sm = plt.cm.ScalarMappable(cmap=\"crest_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labelsDict = {\n",
    "   \"('rating', 10, False)\": \"Rating SOM\", \n",
    "    \"('rating', 5, True)\": \"Rating SOM+Prob, exp:5\", \n",
    "    \"('rating', 10, True)\": \"Rating SOM+Prob, exp:10\", \n",
    "    \"('rating', 20, True)\": \"Rating SOM+Prob, exp:20\", \n",
    "    \"('normal', 10, False)\": \"Plain SOM\", \n",
    "    \"('normal', 5, True)\": \"Plain SOM+Prob, exp:5\", \n",
    "    \"('normal', 10, True)\": \"Plain SOM+Prob, exp:10\", \n",
    "    \"('normal', 20, True)\": \"Plain SOM+Prob, exp:20\", \n",
    "    \n",
    "    \"('topk', 10, False)\": \"Top-k\"     \n",
    "}\n",
    "lbls = [labelsDict.get(i,i) for i in labels[5:]]\n",
    "ax.legend(handles=handles[5:], labels=lbls,bbox_to_anchor=(-0.01,-0.015),loc=3)\n",
    "ax.set_xlabel(\"Ordering relevance (nDCG)\")\n",
    "ax.set_ylabel(\"Overall Diversity\")\n",
    "ax.set_ylim(0.29, 0.58)\n",
    "#ax.set_xlim(0.49, 1.01)\n",
    "\n",
    "ax.figure.colorbar(sm, label=\"Neighbors vs. Overall Diversity Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"scatter5_TriangleEXP.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
